{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416271, 5416271)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Set working directory\n",
    "\n",
    "try:\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "except FileNotFoundError:\n",
    "    os.chdir(\"..\")\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "# Extract edges for \"paper\" -> \"cites\" -> \"paper\"\n",
    "paper_c_paper = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_c_paper[0], nums_train) | torch.isin(paper_c_paper[1], nums_train)\n",
    "mask_valid = torch.isin(paper_c_paper[0], nums_valid) | torch.isin(paper_c_paper[1], nums_valid)\n",
    "mask_test = torch.isin(paper_c_paper[0], nums_test) | torch.isin(paper_c_paper[1], nums_test)\n",
    "\n",
    "paper_c_paper_train = paper_c_paper.clone()\n",
    "paper_c_paper_valid = paper_c_paper.clone()\n",
    "paper_c_paper_test = paper_c_paper.clone()\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_c_paper_train = paper_c_paper_train[:, mask_train_done]\n",
    "paper_c_paper_valid = paper_c_paper_valid[:, mask_valid_done]\n",
    "paper_c_paper_test = paper_c_paper_test[:, mask_test]\n",
    "\n",
    "#Venues\n",
    "venues_values = torch.unique(data['y_dict']['paper'])\n",
    "\n",
    "len(paper_c_paper_train[1]) + len(paper_c_paper_valid[1]) + len(paper_c_paper_test[1]), paper_c_paper.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"dataset/ogbn_mag/processed/paper_embeddings.pt\"):\n",
    "    venue_embeddings = {}\n",
    "    embdding_dim = 2\n",
    "\n",
    "    embed = torch.nn.Embedding(len(venues_values), embdding_dim)\n",
    "\n",
    "    venue_id_to_idx = {venue_id.item(): idx for idx, venue_id in enumerate(venues_values)}\n",
    "\n",
    "    indices = torch.tensor([venue_id_to_idx[venue_id.item()] for venue_id in venues_values], dtype=torch.long)\n",
    "\n",
    "    embeddings = embed(indices)\n",
    "\n",
    "    venue_embeddings = {venue_id.item(): embeddings[venue_id_to_idx[venue_id.item()]].tolist() for venue_id in venues_values}\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    torch.save(venue_embeddings, \"dataset/ogbn_mag/processed/paper_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not os.path.exists(\"dataset/ogbn_mag/processed/paper_embeddings.pt\"):\n",
    "    paper_embeddings = {}\n",
    "    embedding_dim = 2\n",
    "\n",
    "    # Get unique paper IDs\n",
    "    unique_paper_ids = torch.unique(paper_c_paper_train)\n",
    "\n",
    "    # Define the embedding layer (one embedding per unique paper)\n",
    "    embed = torch.nn.Embedding(len(unique_paper_ids), embedding_dim)\n",
    "\n",
    "    # Create a mapping: paper ID → index in embedding layer\n",
    "    paper_id_to_idx = {pid.item(): idx for idx, pid in enumerate(unique_paper_ids)}\n",
    "\n",
    "    # Convert paper_c_paper_train to indices using vectorized operations\n",
    "    indices = torch.tensor([paper_id_to_idx[pid.item()] for pid in paper_c_paper_train.flatten()])\n",
    "\n",
    "    # Compute embeddings\n",
    "    embeddings = embed(indices)\n",
    "\n",
    "    # Convert to dictionary with original paper IDs\n",
    "    paper_embeddings = {pid.item(): emb for pid, emb in zip(paper_c_paper_train.flatten(), embeddings)}\n",
    "\n",
    "    torch.save(venue_embeddings, \"dataset/ogbn_mag/processed/paper_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "class mini_batches_code:\n",
    "    def __init__(self,data, unique_list, sample_size,edge_type):\n",
    "        self.data = data\n",
    "        self.sample_size = sample_size\n",
    "        self.edge_type = edge_type\n",
    "        self.unique_list = unique_list\n",
    "\n",
    "    def get_batch(self):\n",
    "        random.seed(99) \n",
    "        torch.manual_seed(99)\n",
    "        list_pcp = self.unique_list\n",
    "        random_sample = random.sample(list_pcp, self.sample_size)\n",
    "        print(random_sample)\n",
    "        for value in random_sample:\n",
    "            list_pcp.remove(value)\n",
    "        mask = torch.isin(self.data[0], torch.tensor(random_sample))\n",
    "        filtered_data = self.data[:,mask]\n",
    "        return filtered_data, random_sample, list_pcp\n",
    "    \n",
    "    def data_matrix(self):\n",
    "        data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "        edge_entities = {\n",
    "            'paper': 0,\n",
    "            'author': 1,\n",
    "            'institution': 2,\n",
    "            'field_of_study': 3,\n",
    "            'venue': 4,\n",
    "        }\n",
    "        # Get batch and initialize tensors\n",
    "        tensor, random_sample, unique_list = self.get_batch()\n",
    "\n",
    "        # Create result tensor from input batch\n",
    "        result_tensor = torch.stack([torch.tensor([1, tensor[0, i], tensor[1, i],edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]) for i in range(tensor.shape[1])])\n",
    "\n",
    "        # Initialize lists for non_edges and venues\n",
    "        non_edges, venues = [], []\n",
    "\n",
    "        # Add venue links for sampled nodes\n",
    "        for i in random_sample:\n",
    "            venues.append(torch.tensor([1, i.item(), data['y_dict']['paper'][i], edge_entities[self.edge_type[0]],edge_entities['venue']]))\n",
    "\n",
    "            # Find non-existing edges\n",
    "            for j in tensor[1].unique():\n",
    "                if i != j and not torch.any((result_tensor[:, 1] == i) & (result_tensor[:, 2] == j)): \n",
    "                    non_edges.append(torch.tensor([0, i.item(), j.item(),edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]))\n",
    "\n",
    "        for r, j in itertools.combinations(random_sample, 2):  # itertools generates all unique pairs\n",
    "            if data['y_dict']['paper'][r] != data['y_dict']['paper'][j]:\n",
    "                venues.append(torch.tensor([0, r, data['y_dict']['paper'][j],edge_entities['paper'],edge_entities['venue']]))\n",
    "                venues.append(torch.tensor([0, j, data['y_dict']['paper'][r],edge_entities['paper'],edge_entities['venue']]))\n",
    "\n",
    "        # Convert lists to tensors only once to optimize memory usage\n",
    "        non_edges_tensor = torch.stack(non_edges) if non_edges else torch.empty((0, 5), dtype=torch.long)\n",
    "        venues_tensor = torch.stack(venues) if venues else torch.empty((0, 5), dtype=torch.long)\n",
    "\n",
    "        # Merge all tensors\n",
    "        data_matrix = torch.cat((result_tensor, non_edges_tensor, venues_tensor), dim=0)\n",
    "        return data_matrix, unique_list\n",
    "    \n",
    "    def node_mapping(self):\n",
    "\n",
    "        datamatrix_tensor,ul = self.data_matrix()\n",
    "\n",
    "        lm1 = torch.unique(torch.stack((datamatrix_tensor[:, 1], datamatrix_tensor[:, 3]), dim=1), dim=0)\n",
    "        lm2 = torch.unique(torch.stack((datamatrix_tensor[:, 2], datamatrix_tensor[:, 4]), dim=1), dim=0)\n",
    "\n",
    "        unique_global_node_ids = torch.unique(torch.cat([lm1, lm2], dim=0), dim=0)\n",
    "\n",
    "        # Step 2: Create a mapping from global node IDs to local node indices\n",
    "        node_mapping = {(global_id.item(), type_id.item()): idx \n",
    "                            for idx, (global_id, type_id) in enumerate(unique_global_node_ids)}\n",
    "\n",
    "        # Step 3: Remap the indices in the datamatrix_tensor using the node_mapping\n",
    "        # We are remapping columns 1 and 2 in the datamatrix (i.e., the source and destination node indices)\n",
    "        remapped_datamatrix_tensor = datamatrix_tensor.clone()  # Clone the tensor to avoid modifying the original\n",
    "        # Extract the global_id and type_id for remapping\n",
    "        remapped_datamatrix_tensor[:, 1] = torch.tensor([\n",
    "            node_mapping[(global_id.item(), type_id.item())]  \n",
    "            for global_id, type_id in zip(datamatrix_tensor[:, 1], datamatrix_tensor[:, 3])  # Use both columns\n",
    "        ])\n",
    "\n",
    "        remapped_datamatrix_tensor[:, 2] = torch.tensor([\n",
    "            node_mapping[(global_id.item(), type_id.item())]  \n",
    "            for global_id, type_id in zip(datamatrix_tensor[:, 2], datamatrix_tensor[:, 4])  # Use both columns\n",
    "        ])\n",
    "\n",
    "        return datamatrix_tensor, ul, remapped_datamatrix_tensor\n",
    "\n",
    "\n",
    "# mini_b = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "# dm,l1 = mini_b.data_matrix()\n",
    "# mini_b1 = mini_batches_code(paper_c_paper_train, l1, 10,('paper', 'cites', 'paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, alpha=1.0, eps=1e-8, use_regularization=False, lam=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the loss function with given parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): Scaling parameter for edge probability.\n",
    "            eps (float): Small value to prevent log(0).\n",
    "            use_regularization (bool): Whether to include Gaussian regularization.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.use_regularization = use_regularization\n",
    "        self.lam = lam\n",
    "\n",
    "    # def edge_probability(self, z_i, z_j):\n",
    "    #     \"\"\"Compute the probability of an edge existing between two embeddings.\"\"\"\n",
    "    #     dist = torch.norm(z_i - z_j) ** 2  # Squared Euclidean distance\n",
    "    #     return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    # def link_loss(self, label, z_u, z_v):\n",
    "    #     \"\"\"Compute the loss for a single edge.\"\"\"\n",
    "    #     prob = self.edge_probability(z_u, z_v)\n",
    "    #     prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "    #     return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)        \n",
    "\n",
    "    # def compute_loss(self, z, datamatrix_tensor):\n",
    "    #     \"\"\"Compute the total loss for the dataset.\"\"\"\n",
    "    #     sum_loss = sum(\n",
    "    #         self.link_loss(label, z[u_idx], z[v_idx])\n",
    "    #         for label, u_idx, v_idx in datamatrix_tensor\n",
    "    #     )\n",
    "\n",
    "    #     loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "    #     if self.use_regularization:\n",
    "    #         regularization = -0.5 * torch.sum(z ** 2)\n",
    "    #         loss += regularization\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "\n",
    "\n",
    "    def edge_probability(self, z_i, z_j, type_i, type_j):\n",
    "        \"\"\"Compute the probability of an edge existing between two nodes, considering embeddings and types.\"\"\"\n",
    "        type_i = (type_i.view(1, -1).float())\n",
    "        type_j = (type_j.view(1, -1).float())\n",
    "\n",
    "        # Combine the node embeddings and types\n",
    "        z_i = z_i.view(1, -1).float()  # Ensure z_i is a float tensor\n",
    "        z_j = z_j.view(1, -1).float()  # Ensure z_j is a float tensor\n",
    "        \n",
    "        combined_i = torch.cat((z_i, type_i), dim=-1)  # Concatenate embedding and type for node i\n",
    "        combined_j = torch.cat((z_j, type_j), dim=-1)  # Concatenate embedding and type for node j\n",
    "        \n",
    "        dist = torch.norm(combined_i - combined_j) ** 2  # Squared Euclidean distance\n",
    "        return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    def link_loss(self, label, z_u, z_v, type_u, type_v):\n",
    "        \"\"\"Compute the loss for a single edge, considering node types.\"\"\"\n",
    "        prob = self.edge_probability(z_u, z_v, type_u, type_v)\n",
    "        prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "        return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)\n",
    "\n",
    "    def compute_loss(self, z, types, datamatrix_tensor):\n",
    "        \"\"\"Compute the total loss for the dataset, considering node types.\"\"\"\n",
    "        sum_loss = sum(\n",
    "            self.link_loss(label, z[u_idx], z[v_idx], types[u_idx][0], types[v_idx][1])\n",
    "            for label, u_idx, v_idx in datamatrix_tensor)\n",
    "        \n",
    "\n",
    "        loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "        if self.use_regularization:\n",
    "            regularization = self.lam * torch.sum(z ** 2)\n",
    "            loss += regularization\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# loss_fn = LossFunction(alpha=1.0, use_regularization=True)\n",
    "# loss_value = loss_fn.compute_loss(z, datamatrix_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[423601, 399248, 209794, 628557, 187487, 241447, 260499, 139716, 90784, 263350]\n"
     ]
    }
   ],
   "source": [
    "mini_b = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "dm,l1,remapped_datamatrix_tensor = mini_b.node_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 588)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remapped_datamatrix_tensor), len(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.5113\n",
      "Epoch 10: Loss = 2.2348\n",
      "Epoch 20: Loss = 1.9979\n",
      "Epoch 30: Loss = 1.7974\n",
      "Epoch 40: Loss = 1.6259\n",
      "Epoch 50: Loss = 1.4764\n",
      "Epoch 60: Loss = 1.3451\n",
      "Epoch 70: Loss = 1.2307\n",
      "Epoch 80: Loss = 1.1324\n",
      "Epoch 90: Loss = 1.0487\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# dm1 = dm[torch.all(dm[:, 1:] != 90784, dim=1)]\n",
    "\n",
    "datamatrix_tensor = dm\n",
    "num_nodes = len(np.unique(dm[:, 1])) + len(np.unique(dm[:, 2]))\n",
    "# 2️ Define Embeddings\n",
    "embedding_dim = 2\n",
    "node_embeddings = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "optimizer = torch.optim.Adam(node_embeddings.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=True)\n",
    "\n",
    "# 3️ Train Embeddings\n",
    "alpha = 3\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z = node_embeddings.weight  # Get embeddings\n",
    "    types = dm[:,3:]\n",
    "    loss = loss_function.compute_loss(z, types, remapped_datamatrix_tensor[:,:3])  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a sample - needs changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     1,  90784,  44029,      0,      0],\n",
      "        [     1,  90784, 112686,      0,      0],\n",
      "        [     1,  90784, 227830,      0,      0],\n",
      "        [     1,  90784, 267759,      0,      0],\n",
      "        [     1,  90784, 286919,      0,      0],\n",
      "        [     1,  90784, 391810,      0,      0]])\n",
      "tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 2, 0, 0],\n",
      "        [1, 1, 3, 0, 0],\n",
      "        [1, 1, 4, 0, 0],\n",
      "        [1, 1, 5, 0, 0],\n",
      "        [1, 1, 6, 0, 0]])\n",
      "Epoch 0: Loss = 1.9824\n",
      "Epoch 10: Loss = 1.9780\n",
      "Epoch 20: Loss = 1.9740\n",
      "Epoch 30: Loss = 1.9704\n",
      "Epoch 40: Loss = 1.9674\n",
      "Epoch 50: Loss = 1.9648\n",
      "Epoch 60: Loss = 1.9626\n",
      "Epoch 70: Loss = 1.9607\n",
      "Epoch 80: Loss = 1.9591\n",
      "Epoch 90: Loss = 1.9578\n"
     ]
    }
   ],
   "source": [
    "node_embeddings.weight.requires_grad = False\n",
    "emb_matrix = node_embeddings.weight.detach().clone()\n",
    "\n",
    "sample = 1\n",
    "# mini_b_new = mini_batches_code(paper_c_paper_train, l1, sample,('paper', 'cites', 'paper'))\n",
    "# dm_new,l2 = mini_b_new.data_matrix()\n",
    "# new_datamatrix = dm_new[torch.all(dm_new[:, 4:] != 4, dim=1)] #fjerner venues\n",
    "\n",
    "dm_new = dm[(dm[:, 0] == 1) & torch.any(dm == 90784, dim=1)]\n",
    "new_datamatrix = dm_new[torch.all(dm_new[:, 4:] != 4, dim=1)]\n",
    "\n",
    "\n",
    "new_embedding = torch.nn.Embedding(sample, embedding_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(new_embedding.parameters(), lr=0.01)\n",
    "\n",
    "# Step 1: Extract unique global node IDs from the datamatrix\n",
    "unique_global_node_ids_new = torch.unique(new_datamatrix[:, 1],new_datamatrix[:, 3])\n",
    "\n",
    "# Step 2: Create a mapping from global node IDs to local node indices\n",
    "node_mapping_new = {global_id.item(): idx for idx, global_id in enumerate(unique_global_node_ids_new)}\n",
    "\n",
    "# Step 3: Remap the indices in the datamatrix_tensor using the node_mapping_new\n",
    "# We are remapping columns 1 and 2 in the datamatrix (i.e., the source and destination node indices)\n",
    "remapped_datamatrix_tensor_new = new_datamatrix.clone()  # Clone the tensor to avoid modifying the original\n",
    "remapped_datamatrix_tensor_new[:, 1] = torch.tensor([node_mapping_new[global_id.item()] for global_id in new_datamatrix[:, 1]])\n",
    "remapped_datamatrix_tensor_new[:, 2] = torch.tensor([node_mapping_new[global_id.item()] for global_id in new_datamatrix[:, 2]])\n",
    "\n",
    "print(new_datamatrix)\n",
    "print(remapped_datamatrix_tensor_new)\n",
    "\n",
    "alpha = 3\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    types = dm[:,3:]\n",
    "    temp_embeddings = torch.cat([emb_matrix, new_embedding.weight], dim=0)\n",
    "    loss = loss_function.compute_loss(temp_embeddings, types, remapped_datamatrix_tensor_new[:,:3])  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2999, -1.7468])\n",
      "tensor([0.0827, 0.2788])\n",
      "tensor([-0.1822,  0.9507])\n",
      "tensor([ 0.2537, -0.2698])\n",
      "tensor([0.2911, 0.1724])\n",
      "tensor([-1.4072,  1.7272])\n",
      "tensor([ 0.2714, -0.4648])\n",
      "tensor([ 0.2601, -0.0051])\n",
      "tensor([-0.1245, -1.1417])\n"
     ]
    }
   ],
   "source": [
    "test1 = dm1[dm1[:, 4] == 4]\n",
    "for i in list(torch.unique(test1[:,2]).numpy()):\n",
    "    global_node_id = i  # Example global node ID\n",
    "    local_index = node_mapping.get(global_node_id, None)\n",
    "    if local_index is not None:\n",
    "        embedding = emb_matrix[local_index]\n",
    "        print(embedding)\n",
    "    else:\n",
    "        print(\"Node ID not found in mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207,\n",
       " [(1.0306346416473389, 49),\n",
       "  (1.0256558656692505, 51),\n",
       "  (1.713150143623352, 108),\n",
       "  (0.5174657702445984, 109),\n",
       "  (0.9493674635887146, 128),\n",
       "  (2.871931314468384, 134),\n",
       "  (0.3568853437900543, 207),\n",
       "  (0.7699180245399475, 214),\n",
       "  (0.4329391419887543, 219)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = []\n",
    "for i in list(torch.unique(test1[:,2]).numpy()):\n",
    "    global_node_id = i  # Example global node ID\n",
    "    local_index = node_mapping.get(global_node_id, None)\n",
    "    dist = torch.norm(new_embedding.weight - emb_matrix[local_index])  # Euclidean distance\n",
    "    distances.append((dist.item(), i))\n",
    "\n",
    "# 8️ Assign B_new to closest C node\n",
    "min(distances, key=lambda x: x[0])[1], distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Node ID: 207\n",
      "Highest Softmax Probability: 0.13181227445602417\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "alpha = 0.001\n",
    "logi_f = []\n",
    "\n",
    "for i in list(torch.unique(test1[:, 2]).numpy()):\n",
    "    global_node_id = i  # Example global node ID\n",
    "    local_index = node_mapping.get(global_node_id, None)\n",
    "    \n",
    "    if local_index is not None:  # Ensure the index exists\n",
    "        dist = torch.norm(new_embedding.weight - emb_matrix[local_index])**2  # Euclidean distance\n",
    "        logi = 1 / (1 + torch.exp(alpha + dist))  # Logistic function\n",
    "        logi_f.append((logi.item(), i))  # Store tuple (probability, node ID)\n",
    "\n",
    "# Separate values for softmax computation\n",
    "logits, node_ids = zip(*logi_f)  # Unzips into two lists\n",
    "\n",
    "# Convert logits to a tensor and apply softmax\n",
    "logi_f_tensor = torch.tensor(logits)\n",
    "softma = F.softmax(logi_f_tensor, dim=0)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "high_prob_idx = torch.argmax(softma).item()\n",
    "\n",
    "# Get the corresponding node ID and its softmax probability\n",
    "predicted_node_id = node_ids[high_prob_idx]\n",
    "highest_prob_value = softma[high_prob_idx].item()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Node ID: {predicted_node_id}\")\n",
    "print(f\"Highest Softmax Probability: {highest_prob_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[423601, 399248, 209794, 628557, 187487, 241447, 260499, 139716, 90784, 263350]\n"
     ]
    }
   ],
   "source": [
    "mini_b_multi1 = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "dm_multi1,l2,remapped_datamatrix_tensor_multi1 = mini_b_multi1.node_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[423610, 399256, 209798, 628567, 187490, 241452, 260505, 139718, 90785, 263357]\n"
     ]
    }
   ],
   "source": [
    "mini_b_multi2 = mini_batches_code(paper_c_paper_train, l2, 10,('paper', 'cites', 'paper'))\n",
    "dm_multi2,l3,remapped_datamatrix_tensor_multi2 = mini_b_multi2.node_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.5113\n",
      "Epoch 10: Loss = 2.2348\n",
      "Epoch 20: Loss = 1.9979\n",
      "Epoch 30: Loss = 1.7974\n",
      "Epoch 40: Loss = 1.6259\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# dm1 = dm_multi1[torch.all(dm_multi1[:, 1:] != 90784, dim=1)]\n",
    "\n",
    "datamatrix_tensor_multi1 = dm_multi1\n",
    "num_nodes = len(np.unique(dm_multi1[:, 1])) + len(np.unique(dm_multi1[:, 2]))\n",
    "# 2️ Define Embeddings\n",
    "embedding_dim = 2\n",
    "node_embeddings_multi1 = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "optimizer = torch.optim.Adam(node_embeddings_multi1.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=True)\n",
    "\n",
    "# 3️ Train Embeddings\n",
    "alpha = 3\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z = node_embeddings_multi1.weight  # Get embeddings\n",
    "    types = dm_multi1[:,3:]\n",
    "    loss = loss_function.compute_loss(z, types, remapped_datamatrix_tensor_multi1[:,:3])  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.8094\n",
      "Epoch 10: Loss = 2.4676\n",
      "Epoch 20: Loss = 2.1712\n",
      "Epoch 30: Loss = 1.9151\n",
      "Epoch 40: Loss = 1.6951\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# dm1 = dm_multi2[torch.all(dm_multi2[:, 1:] != 90784, dim=1)]\n",
    "\n",
    "datamatrix_tensor_multi2 = dm_multi2\n",
    "num_nodes = len(np.unique(dm_multi2[:, 1])) + len(np.unique(dm_multi2[:, 2]))\n",
    "# 2️ Define Embeddings\n",
    "embedding_dim = 2\n",
    "node_embeddings_multi2 = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "optimizer = torch.optim.Adam(node_embeddings_multi2.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=True)\n",
    "\n",
    "# 3️ Train Embeddings\n",
    "alpha = 3\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z = node_embeddings_multi2.weight  # Get embeddings\n",
    "    types = dm_multi2[:,3:]\n",
    "    loss = loss_function.compute_loss(z, types, remapped_datamatrix_tensor_multi2[:,:3])  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
