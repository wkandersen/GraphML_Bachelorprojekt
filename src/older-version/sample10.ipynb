{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416271, 5416271)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Set working directory\n",
    "\n",
    "try:\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "except FileNotFoundError:\n",
    "    os.chdir(\"..\")\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "# Extract edges for \"paper\" -> \"cites\" -> \"paper\"\n",
    "paper_c_paper = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_c_paper[0], nums_train) | torch.isin(paper_c_paper[1], nums_train)\n",
    "mask_valid = torch.isin(paper_c_paper[0], nums_valid) | torch.isin(paper_c_paper[1], nums_valid)\n",
    "mask_test = torch.isin(paper_c_paper[0], nums_test) | torch.isin(paper_c_paper[1], nums_test)\n",
    "\n",
    "paper_c_paper_train = paper_c_paper.clone()\n",
    "paper_c_paper_valid = paper_c_paper.clone()\n",
    "paper_c_paper_test = paper_c_paper.clone()\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_c_paper_train = paper_c_paper_train[:, mask_train_done]\n",
    "paper_c_paper_valid = paper_c_paper_valid[:, mask_valid_done]\n",
    "paper_c_paper_test = paper_c_paper_test[:, mask_test]\n",
    "\n",
    "#Venues\n",
    "venues_values = torch.unique(data['y_dict']['paper'])\n",
    "\n",
    "len(paper_c_paper_train[1]) + len(paper_c_paper_valid[1]) + len(paper_c_paper_test[1]), paper_c_paper.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if not os.path.exists(\"dataset/ogbn_mag/processed/venue_embeddings.pt\"):\n",
    "#     venue_embeddings = {}\n",
    "#     embdding_dim = 2\n",
    "\n",
    "#     embed = torch.nn.Embedding(len(venues_values), embdding_dim)\n",
    "\n",
    "#     venue_id_to_idx = {venue_id.item(): idx for idx, venue_id in enumerate(venues_values)}\n",
    "\n",
    "#     indices = torch.tensor([venue_id_to_idx[venue_id.item()] for venue_id in venues_values], dtype=torch.long)\n",
    "\n",
    "#     embeddings = embed(indices)\n",
    "\n",
    "#     venue_embeddings = {venue_id.item(): embeddings[venue_id_to_idx[venue_id.item()]] for venue_id in venues_values}\n",
    "\n",
    "#     # Save the embeddings to a file\n",
    "#     torch.save(venue_embeddings, \"dataset/ogbn_mag/processed/venue_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if not os.path.exists(\"dataset/ogbn_mag/processed/paper_embeddings.pt\"):\n",
    "#     paper_embeddings = {}\n",
    "#     embedding_dim = 2\n",
    "\n",
    "#     # Get unique paper IDs\n",
    "#     unique_paper_ids = torch.unique(paper_c_paper_train)\n",
    "\n",
    "#     # Define the embedding layer (one embedding per unique paper)\n",
    "#     embed = torch.nn.Embedding(len(unique_paper_ids), embedding_dim)\n",
    "\n",
    "#     # Create a mapping: paper ID â†’ index in embedding layer\n",
    "#     paper_id_to_idx = {pid.item(): idx for idx, pid in enumerate(unique_paper_ids)}\n",
    "\n",
    "#     # Convert paper_c_paper_train to indices using vectorized operations\n",
    "#     indices = torch.tensor([paper_id_to_idx[pid.item()] for pid in paper_c_paper_train.flatten()])\n",
    "\n",
    "#     # Compute embeddings\n",
    "#     embeddings = embed(indices)\n",
    "\n",
    "#     # Convert to dictionary with original paper IDs\n",
    "#     paper_embeddings = {pid.item(): emb for pid, emb in zip(paper_c_paper_train.flatten(), embeddings)}\n",
    "\n",
    "#     torch.save(venue_embeddings, \"dataset/ogbn_mag/processed/paper_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "class mini_batches_code:\n",
    "    def __init__(self,data, unique_list, sample_size,edge_type):\n",
    "        self.data = data\n",
    "        self.sample_size = sample_size\n",
    "        self.edge_type = edge_type\n",
    "        self.unique_list = unique_list\n",
    "\n",
    "    def get_batch(self):\n",
    "        random.seed(99) \n",
    "        torch.manual_seed(99)\n",
    "        list_pcp = self.unique_list\n",
    "        random_sample = random.sample(list_pcp, self.sample_size)\n",
    "        print(random_sample)\n",
    "        for value in random_sample:\n",
    "            list_pcp.remove(value)\n",
    "        mask = torch.isin(self.data[0], torch.tensor(random_sample))\n",
    "        filtered_data = self.data[:,mask]\n",
    "        return filtered_data, random_sample, list_pcp\n",
    "    \n",
    "    def data_matrix(self):\n",
    "        data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "        edge_entities = {\n",
    "            'paper': 0,\n",
    "            'author': 1,\n",
    "            'institution': 2,\n",
    "            'field_of_study': 3,\n",
    "            'venue': 4,\n",
    "        }\n",
    "        # Get batch and initialize tensors\n",
    "        tensor, random_sample, unique_list = self.get_batch()\n",
    "\n",
    "        # Create result tensor from input batch\n",
    "        result_tensor = torch.stack([torch.tensor([1, tensor[0, i], tensor[1, i],edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]) for i in range(tensor.shape[1])])\n",
    "\n",
    "        # Initialize lists for non_edges and venues\n",
    "        non_edges, venues = [], []\n",
    "\n",
    "        # Add venue links for sampled nodes\n",
    "        for i in random_sample:\n",
    "            venues.append(torch.tensor([1, i.item(), data['y_dict']['paper'][i], edge_entities[self.edge_type[0]],edge_entities['venue']]))\n",
    "\n",
    "            # Find non-existing edges\n",
    "            for j in tensor[1].unique():\n",
    "                if i != j and not torch.any((result_tensor[:, 1] == i) & (result_tensor[:, 2] == j)): \n",
    "                    non_edges.append(torch.tensor([0, i.item(), j.item(),edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]))\n",
    "\n",
    "        for r, j in itertools.combinations(random_sample, 2):  # itertools generates all unique pairs\n",
    "            if data['y_dict']['paper'][r] != data['y_dict']['paper'][j]:\n",
    "                venues.append(torch.tensor([0, r, data['y_dict']['paper'][j],edge_entities['paper'],edge_entities['venue']]))\n",
    "                venues.append(torch.tensor([0, j, data['y_dict']['paper'][r],edge_entities['paper'],edge_entities['venue']]))\n",
    "\n",
    "        # Convert lists to tensors only once to optimize memory usage\n",
    "        non_edges_tensor = torch.stack(non_edges) if non_edges else torch.empty((0, 5), dtype=torch.long)\n",
    "        venues_tensor = torch.stack(venues) if venues else torch.empty((0, 5), dtype=torch.long)\n",
    "\n",
    "        # Merge all tensors\n",
    "        data_matrix = torch.cat((result_tensor, non_edges_tensor, venues_tensor), dim=0)\n",
    "        return data_matrix, unique_list\n",
    "    \n",
    "    def node_mapping(self):\n",
    "\n",
    "        datamatrix_tensor,ul = self.data_matrix()\n",
    "\n",
    "        lm1 = torch.unique(torch.stack((datamatrix_tensor[:, 1], datamatrix_tensor[:, 3]), dim=1), dim=0)\n",
    "        lm2 = torch.unique(torch.stack((datamatrix_tensor[:, 2], datamatrix_tensor[:, 4]), dim=1), dim=0)\n",
    "\n",
    "        unique_global_node_ids = torch.unique(torch.cat([lm1, lm2], dim=0), dim=0)\n",
    "\n",
    "        # Step 2: Create a mapping from global node IDs to local node indices\n",
    "        node_mapping = {(global_id.item(), type_id.item()): idx \n",
    "                            for idx, (global_id, type_id) in enumerate(unique_global_node_ids)}\n",
    "\n",
    "        # Step 3: Remap the indices in the datamatrix_tensor using the node_mapping\n",
    "        # We are remapping columns 1 and 2 in the datamatrix (i.e., the source and destination node indices)\n",
    "        remapped_datamatrix_tensor = datamatrix_tensor.clone()  # Clone the tensor to avoid modifying the original\n",
    "        # Extract the global_id and type_id for remapping\n",
    "        remapped_datamatrix_tensor[:, 1] = torch.tensor([\n",
    "            node_mapping[(global_id.item(), type_id.item())]  \n",
    "            for global_id, type_id in zip(datamatrix_tensor[:, 1], datamatrix_tensor[:, 3])  # Use both columns\n",
    "        ])\n",
    "\n",
    "        remapped_datamatrix_tensor[:, 2] = torch.tensor([\n",
    "            node_mapping[(global_id.item(), type_id.item())]  \n",
    "            for global_id, type_id in zip(datamatrix_tensor[:, 2], datamatrix_tensor[:, 4])  # Use both columns\n",
    "        ])\n",
    "\n",
    "        return datamatrix_tensor, ul, remapped_datamatrix_tensor\n",
    "\n",
    "\n",
    "# mini_b = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "# dm,l1 = mini_b.data_matrix()\n",
    "# mini_b1 = mini_batches_code(paper_c_paper_train, l1, 10,('paper', 'cites', 'paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, alpha=1.0, eps=1e-8, use_regularization=False, lam=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the loss function with given parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): Scaling parameter for edge probability.\n",
    "            eps (float): Small value to prevent log(0).\n",
    "            use_regularization (bool): Whether to include Gaussian regularization.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.use_regularization = use_regularization\n",
    "        self.lam = lam\n",
    "\n",
    "    def edge_probability(self, z_i, z_j):\n",
    "        \"\"\"Compute the probability of an edge existing between two embeddings.\"\"\"\n",
    "        dist = torch.norm(z_i - z_j) ** 2  # Squared Euclidean distance\n",
    "        return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    def link_loss(self, label, z_u, z_v):\n",
    "        \"\"\"Compute the loss for a single edge.\"\"\"\n",
    "        prob = self.edge_probability(z_u, z_v)\n",
    "        prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "        return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)        \n",
    "\n",
    "    def compute_loss(self, z, datamatrix_tensor):\n",
    "        \"\"\"Compute the total loss for the dataset.\"\"\"\n",
    "        sum_loss = sum(\n",
    "            self.link_loss(label, z[u_idx], z[v_idx])\n",
    "            for label, u_idx, v_idx in datamatrix_tensor\n",
    "        )\n",
    "\n",
    "        loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "        if self.use_regularization:\n",
    "            regularization = -self.lam * torch.sum(z ** 2)\n",
    "            loss += regularization\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "#     def edge_probability(self, z_i, z_j, type_i, type_j):\n",
    "#         \"\"\"Compute the probability of an edge existing between two nodes, considering embeddings and types.\"\"\"\n",
    "#         type_i = (type_i.view(1, -1).float())*0\n",
    "#         type_j = (type_j.view(1, -1).float())*0\n",
    "\n",
    "#         # Combine the node embeddings and types\n",
    "#         z_i = z_i.view(1, -1).float()  # Ensure z_i is a float tensor\n",
    "#         z_j = z_j.view(1, -1).float()  # Ensure z_j is a float tensor\n",
    "        \n",
    "#         combined_i = torch.cat((z_i, type_i), dim=-1)  # Concatenate embedding and type for node i\n",
    "#         combined_j = torch.cat((z_j, type_j), dim=-1)  # Concatenate embedding and type for node j\n",
    "        \n",
    "#         dist = torch.norm(combined_i - combined_j) ** 2  # Squared Euclidean distance\n",
    "#         return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "#     def link_loss(self, label, z_u, z_v, type_u, type_v):\n",
    "#         \"\"\"Compute the loss for a single edge, considering node types.\"\"\"\n",
    "#         prob = self.edge_probability(z_u, z_v, type_u, type_v)\n",
    "#         prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "#         return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)\n",
    "\n",
    "#     def compute_loss(self, z, types, datamatrix_tensor):\n",
    "#         \"\"\"Compute the total loss for the dataset, considering node types.\"\"\"\n",
    "#         sum_loss = sum(\n",
    "#             self.link_loss(label, z[u_idx], z[v_idx], types[u_idx][0], types[v_idx][1])\n",
    "#             for label, u_idx, v_idx in datamatrix_tensor)\n",
    "        \n",
    "\n",
    "#         loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "#         if self.use_regularization:\n",
    "#             regularization = self.lam * torch.sum(z ** 2)\n",
    "#             loss += regularization\n",
    "\n",
    "#         return loss\n",
    "    \n",
    "\n",
    "# # loss_fn = LossFunction(alpha=1.0, use_regularization=True)\n",
    "# # loss_value = loss_fn.compute_loss(z, datamatrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from LossFunction import LossFunction\n",
    "\n",
    "class NodeEmbeddingTrainer:\n",
    "    def __init__(self, dm, remapped_datamatrix_tensor, paper_dict, venue_dict, embedding_dim=2, num_epochs=50, lr=0.01, alpha=3):\n",
    "        # Initialize input data, parameters, and setup\n",
    "        self.dm = dm\n",
    "        self.remapped_datamatrix_tensor = remapped_datamatrix_tensor\n",
    "        self.paper_dict = paper_dict\n",
    "        self.venue_dict = venue_dict\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "\n",
    "        # Process data\n",
    "        self.dm1 = dm[dm[:, 4] != 4]\n",
    "        self.dm2 = dm[dm[:, 4] == 4]\n",
    "\n",
    "        # Get node indices\n",
    "        self.specific_papernode_indices = torch.cat([torch.unique(self.dm1[:, 1]), torch.unique(self.dm1[:, 2])], dim=0)\n",
    "        self.specific_venuenode_indices = torch.unique(self.dm2[:, 2], dim=0)\n",
    "\n",
    "        # Create embeddings\n",
    "        self.papernode_embeddings = torch.nn.Embedding(len(self.specific_papernode_indices), self.embedding_dim)\n",
    "        self.venuenode_embeddings = torch.nn.Embedding(len(self.specific_venuenode_indices), self.embedding_dim)\n",
    "\n",
    "        # Optimizers\n",
    "        self.paper_optimizer = torch.optim.Adam(self.papernode_embeddings.parameters(), lr=self.lr)\n",
    "        self.venue_optimizer = torch.optim.Adam(self.venuenode_embeddings.parameters(), lr=self.lr)\n",
    "\n",
    "        # Loss function (assumed to be defined elsewhere)\n",
    "        self.loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=True)\n",
    "\n",
    "    def train(self):\n",
    "        venue_dict = self.venue_dict\n",
    "        paper_dict = self.paper_dict\n",
    "        # Training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.paper_optimizer.zero_grad()\n",
    "            self.venue_optimizer.zero_grad()\n",
    "\n",
    "            # Concatenate the embeddings\n",
    "            z = torch.cat((self.papernode_embeddings.weight, self.venuenode_embeddings.weight), dim=0)\n",
    "            types = self.dm[:, 3:]\n",
    "            loss = self.loss_function.compute_loss(z, types, self.remapped_datamatrix_tensor[:, :3])  # Compute loss\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            self.paper_optimizer.step()\n",
    "            self.venue_optimizer.step()\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        print(self.specific_venuenode_indices)\n",
    "\n",
    "        for idx, node in enumerate(self.specific_papernode_indices):\n",
    "            paper_dict[int(node)] = self.papernode_embeddings.weight[idx]\n",
    "\n",
    "        for idx, node in enumerate(self.specific_venuenode_indices):\n",
    "            venue_dict[int(node)] = self.venuenode_embeddings.weight[idx]\n",
    "            \n",
    "        return paper_dict, venue_dict\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "\n",
    "# # Assuming 'dm' is your data matrix\n",
    "# trainer = NodeEmbeddingTrainer(dm, embedding_dim=2, num_epochs=10, lr=0.01, alpha=3)\n",
    "# trainer.train()  # Train embeddings\n",
    "\n",
    "# # Get the resulting dictionaries with embeddings\n",
    "# paper_dict, venue_dict = trainer.get_embeddings()\n",
    "\n",
    "# # Optionally: Print the dictionaries\n",
    "# print(\"Paper Embeddings:\", paper_dict)\n",
    "# print(\"Venue Embeddings:\", venue_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30677/1424518279.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embed_venue = torch.load(\"/mnt/c/Users/Bruger/Desktop/Bachelor/GraphML_Bachelorprojekt/dataset/ogbn_mag/processed/venue_embeddings.pt\")\n",
      "/tmp/ipykernel_30677/1424518279.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embed_paper = torch.load(\"/mnt/c/Users/Bruger/Desktop/Bachelor/GraphML_Bachelorprojekt/dataset/ogbn_mag/processed/paper_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "# Load initial embeddings\n",
    "embed_venue = torch.load(\"/mnt/c/Users/Bruger/Desktop/Bachelor/GraphML_Bachelorprojekt/dataset/ogbn_mag/processed/venue_embeddings.pt\")\n",
    "embed_paper = torch.load(\"/mnt/c/Users/Bruger/Desktop/Bachelor/GraphML_Bachelorprojekt/dataset/ogbn_mag/processed/paper_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[427176, 402674, 211669, 189176, 243585]\n",
      "Epoch 0: Loss = 2.4848\n",
      "Epoch 10: Loss = 2.1904\n",
      "Epoch 20: Loss = 1.9335\n",
      "Epoch 30: Loss = 1.7108\n",
      "Epoch 40: Loss = 1.5196\n",
      "tensor([  1, 100, 134, 250, 277])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "# # Load initial embeddings\n",
    "# embed_venue = torch.load(\"dataset/ogbn_mag/processed/venue_embeddings.pt\")\n",
    "# embed_paper = torch.load(\"dataset/ogbn_mag/processed/paper_embeddings.pt\")\n",
    "\n",
    "# Initialize dictionaries to store embeddings\n",
    "paper_dict = copy.deepcopy(embed_paper)  # Ensure we don't modify the original embeddings\n",
    "venue_dict = copy.deepcopy(embed_venue)\n",
    "l_prev = list(paper_c_paper_train.unique().numpy())  # Initial list of nodes\n",
    "\n",
    "# Number of iterations (adjust as needed)\n",
    "num_iterations = 1 \n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Generate mini-batches\n",
    "    mini_b = mini_batches_code(paper_c_paper_train, l_prev, 5, ('paper', 'cites', 'paper'))\n",
    "    dm, l_next, remapped_datamatrix_tensor = mini_b.node_mapping()\n",
    "\n",
    "    # Train embeddings and update dictionaries **in place**\n",
    "    N_emb = NodeEmbeddingTrainer(\n",
    "        dm=dm,\n",
    "        remapped_datamatrix_tensor=remapped_datamatrix_tensor,\n",
    "        paper_dict=paper_dict,  # Pass reference (no copy)\n",
    "        venue_dict=venue_dict\n",
    "    )\n",
    "    paper_dict, venue_dict = N_emb.train()  # Directly update original dictionaries\n",
    "\n",
    "    # Update node list for the next iteration\n",
    "    l_prev = l_next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a sample - needs changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in paper_dict:\n",
    "    paper_dict[key] = paper_dict[key].detach().clone()\n",
    "    paper_dict[key].requires_grad = False  # Ensure no gradients are tracked\n",
    "\n",
    "for key in venue_dict:\n",
    "    venue_dict[key] = venue_dict[key].detach().clone()\n",
    "    venue_dict[key].requires_grad = False  # Ensure no gradients are tracked\n",
    "\n",
    "emb_matrix = torch.stack(list(paper_dict.values()) + list(venue_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[427176]\n",
      "Parameter containing:\n",
      "tensor([[ 0.6127, -1.1754]], requires_grad=True)\n",
      "623918 1\n",
      "Epoch 0: Loss = -12479.9619\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "Epoch 10: Loss = -12479.9648\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n",
      "623918 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample = 1\n",
    "mini_b_new = mini_batches_code(paper_c_paper_train, list(paper_c_paper_train.unique().numpy()), sample,('paper', 'cites', 'paper'))\n",
    "dm_new,l_new,remapped_datamatrix_tensor_new = mini_b_new.node_mapping()\n",
    "new_datamatrix = dm_new[torch.all(dm_new[:, 4:] != 4, dim=1)]\n",
    "new_remapped_datamatrix_tensor_new = remapped_datamatrix_tensor_new[torch.all(remapped_datamatrix_tensor_new[:, 4:] != 4, dim=1)]\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=True)\n",
    "\n",
    "new_embedding = torch.nn.Embedding(sample, 2)\n",
    "print(new_embedding.weight)\n",
    "\n",
    "new_optimizer = torch.optim.Adam(new_embedding.parameters(), lr=0.01)\n",
    "\n",
    "venue_dict = venue_dict.copy()\n",
    "paper_dict = paper_dict.copy()\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    new_optimizer.zero_grad()\n",
    "\n",
    "    # Concatenate the embeddings\n",
    "    temp_embed = torch.cat([emb_matrix, new_embedding.weight], dim=0)\n",
    "    # types = new_datamatrix[:, 3:]\n",
    "    print(len(temp_embed),len(new_remapped_datamatrix_tensor_new))\n",
    "    loss = loss_function.compute_loss(temp_embed, new_remapped_datamatrix_tensor_new[:, :3])  # Compute loss\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    new_optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8159, -1.3772]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Node ID: 322\n",
      "Highest Softmax Probability: 0.004016751889139414\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "alpha = 0.001\n",
    "logi_f = []\n",
    "\n",
    "for i in range(len(venue_dict)):\n",
    "        dist = torch.norm(new_embedding.weight - venue_dict[i])**2  # Euclidean distance\n",
    "        logi = 1 / (1 + torch.exp(alpha + dist))  # Logistic function\n",
    "        logi_f.append((logi.item(), i))  # Store tuple (probability, node ID)\n",
    "\n",
    "# Separate values for softmax computation\n",
    "logits, node_ids = zip(*logi_f)  # Unzips into two lists\n",
    "\n",
    "# Convert logits to a tensor and apply softmax\n",
    "logi_f_tensor = torch.tensor(logits)\n",
    "softma = F.softmax(logi_f_tensor, dim=0)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "high_prob_idx = torch.argmax(softma).item()\n",
    "\n",
    "# Get the corresponding node ID and its softmax probability\n",
    "predicted_node_id = node_ids[high_prob_idx]\n",
    "highest_prob_value = softma[high_prob_idx].item()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Node ID: {predicted_node_id}\")\n",
    "print(f\"Highest Softmax Probability: {highest_prob_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0026)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softma[232]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
