{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416271, 5416271)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Set working directory\n",
    "\n",
    "try:\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "except FileNotFoundError:\n",
    "    os.chdir(\"..\")\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "# Extract edges for \"paper\" -> \"cites\" -> \"paper\"\n",
    "paper_c_paper = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_c_paper[0], nums_train) | torch.isin(paper_c_paper[1], nums_train)\n",
    "mask_valid = torch.isin(paper_c_paper[0], nums_valid) | torch.isin(paper_c_paper[1], nums_valid)\n",
    "mask_test = torch.isin(paper_c_paper[0], nums_test) | torch.isin(paper_c_paper[1], nums_test)\n",
    "\n",
    "paper_c_paper_train = paper_c_paper.clone()\n",
    "paper_c_paper_valid = paper_c_paper.clone()\n",
    "paper_c_paper_test = paper_c_paper.clone()\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_c_paper_train = paper_c_paper_train[:, mask_train_done]\n",
    "paper_c_paper_valid = paper_c_paper_valid[:, mask_valid_done]\n",
    "paper_c_paper_test = paper_c_paper_test[:, mask_test]\n",
    "\n",
    "len(paper_c_paper_train[1]) + len(paper_c_paper_valid[1]) + len(paper_c_paper_test[1]), paper_c_paper.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7145660, 7145660)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_w_paper = data.edge_index_dict[('author', 'writes', 'paper')]\n",
    "\n",
    "author_w_paper_train = author_w_paper.clone()\n",
    "author_w_paper_valid = author_w_paper.clone()\n",
    "author_w_paper_test = author_w_paper.clone()\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(author_w_paper[1], nums_train)\n",
    "mask_valid = torch.isin(author_w_paper[1], nums_valid)\n",
    "mask_test = torch.isin(author_w_paper[1], nums_test)\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "author_w_paper_train = author_w_paper_train[:, mask_train_done]\n",
    "author_w_paper_valid = author_w_paper_valid[:, mask_valid_done]\n",
    "author_w_paper_test = author_w_paper_test[:, mask_test]\n",
    "\n",
    "len(author_w_paper_train[1]) + len(author_w_paper_valid[1]) + len(author_w_paper_test[1]), author_w_paper.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7505078, 7505078)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_t_field = data.edge_index_dict[('paper', 'has_topic', 'field_of_study')]\n",
    "\n",
    "paper_t_field_train = paper_t_field.clone()\n",
    "paper_t_field_valid = paper_t_field.clone()\n",
    "paper_t_field_test = paper_t_field.clone()\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_t_field[0], nums_train)\n",
    "mask_valid = torch.isin(paper_t_field[0], nums_valid)\n",
    "mask_test = torch.isin(paper_t_field[0], nums_test)\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_t_field_train = paper_t_field_train[:, mask_train_done]\n",
    "paper_t_field_valid = paper_t_field_valid[:, mask_valid_done]\n",
    "paper_t_field_test = paper_t_field_test[:, mask_test]\n",
    "\n",
    "len(paper_t_field_train[1]) + len(paper_t_field_valid[1]) + len(paper_t_field_test[1]), paper_t_field.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([True, True, True,  ..., True, True, True])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#convert to tensor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m paper_y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue[mask_train_done]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([True, True, True,  ..., True, True, True])"
     ]
    }
   ],
   "source": [
    "data[\"y_dict\"][\"paper\"] = pd.DataFrame(data[\"y_dict\"][\"paper\"]).reset_index()\n",
    "data[\"y_dict\"][\"paper\"].columns = ['index', 'value']\n",
    "\n",
    "#convert to tensor\n",
    "paper_y = torch.tensor(data[\"y_dict\"][\"paper\"]['value'].values)\n",
    "paper_y_train = paper_y.clone()\n",
    "paper_y_valid = paper_y.clone()\n",
    "paper_y_test = paper_y.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([189]), tensor([9]), tensor([95]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"y_dict\"][\"paper\"][2],data[\"y_dict\"][\"paper\"][5],data[\"y_dict\"][\"paper\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94203/2906774071.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  paper_c_paper_train_0 = np.array(paper_c_paper_train[0])\n",
      "/tmp/ipykernel_94203/2906774071.py:4: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  paper_c_paper_train_1 = np.array(paper_c_paper_train[1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3879967, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "paper_c_paper_train_0 = np.array(paper_c_paper_train[0])\n",
    "paper_c_paper_train_1 = np.array(paper_c_paper_train[1])\n",
    "\n",
    "datamatrix = np.column_stack([\n",
    "    (paper_c_paper_train_0 == np.arange(len(paper_c_paper_train_0))).astype(int), paper_c_paper_train_0, paper_c_paper_train_1, \n",
    "])\n",
    "\n",
    "# np.argwhere(d[0, 1] == 0)\n",
    "len(np.argwhere(datamatrix[:, 0] == 0)), len(np.argwhere(datamatrix[:, 0] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ...,    184,    184,    185],\n",
       "        [    88,  27449, 121051,  ..., 497699, 720312,  96064]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "\n",
    "# only use the first 1000 papers\n",
    "max_paper_id = 1000\n",
    "paper_c_paper_train_sampled = paper_c_paper_train[:, :max_paper_id]\n",
    "\n",
    "# Example citation data: Replace with your actual data\n",
    "# For example:\n",
    "# paper_ids = [0, 88, 27449, 121051, ...]  # Citing papers\n",
    "# cited_ids = [88, 27449, 121051, ..., 421711, 427339, 439864]  # Cited papers\n",
    "\n",
    "# Example tensor as you mentioned (2 rows, multiple columns)\n",
    "# tensor[0] represents paper_ids (papers that cite others)\n",
    "# tensor[1] represents cited paper_ids (papers being cited)\n",
    "tensor = paper_c_paper_train_sampled\n",
    "\n",
    "# Extract the data\n",
    "paper_ids = tensor[0].numpy()  # Citing papers\n",
    "cited_ids = tensor[1].numpy()  # Cited papers\n",
    "\n",
    "# Create a sparse matrix in COO format:\n",
    "citation_values = np.ones(len(paper_ids))  # All citations will have value 1\n",
    "\n",
    "# Create the sparse matrix with the correct shape\n",
    "citation_matrix_coo = coo_matrix((citation_values, (paper_ids, cited_ids)), shape=(max(paper_ids) + 1, max(cited_ids) + 1))\n",
    "\n",
    "# Now you have a sparse matrix with 3 columns (paper_id, cited_paper_id, 1 for citation)\n",
    "# Optionally, you can convert it to CSR format for better performance:\n",
    "citation_matrix_csr = citation_matrix_coo.tocsr()\n",
    "\n",
    "# If you want to save this matrix to disk (e.g., in HDF5 format), you can do so:\n",
    "# import h5py\n",
    "# with h5py.File('citation_matrix.h5', 'w') as f:\n",
    "#     f.create_dataset('rows', data=citation_matrix_coo.row)\n",
    "#     f.create_dataset('cols', data=citation_matrix_coo.col)\n",
    "#     f.create_dataset('data', data=citation_matrix_coo.data)\n",
    "\n",
    "\n",
    "# use in pytorch\n",
    "citation_matrix = torch.sparse_coo_tensor((citation_matrix_coo.row, citation_matrix_coo.col), citation_matrix_coo.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:44731 [kernel]\nMeta: registered at /pytorch/build/aten/src/ATen/RegisterMeta.cpp:27006 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:955 [kernel]\nQuantizedCUDA: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCUDA.cpp:463 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:23301 [kernel]\nNamed: fallthrough registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /pytorch/build/aten/src/ATen/RegisterZeroTensor.cpp:165 [kernel]\nADInplaceOrView: registered at /pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:17100 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Use in PyTorch\u001b[39;00m\n\u001b[1;32m     32\u001b[0m citation_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m     33\u001b[0m     (citation_matrix_coo\u001b[38;5;241m.\u001b[39mrow, citation_matrix_coo\u001b[38;5;241m.\u001b[39mcol), citation_matrix_coo\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m citation_matrix[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:44731 [kernel]\nMeta: registered at /pytorch/build/aten/src/ATen/RegisterMeta.cpp:27006 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:955 [kernel]\nQuantizedCUDA: registered at /pytorch/build/aten/src/ATen/RegisterQuantizedCUDA.cpp:463 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:23301 [kernel]\nNamed: fallthrough registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /pytorch/build/aten/src/ATen/RegisterZeroTensor.cpp:165 [kernel]\nADInplaceOrView: registered at /pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:18082 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:17100 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "\n",
    "# Only use the first 1000 papers\n",
    "max_paper_id = 1000\n",
    "paper_c_paper_train_sampled = paper_c_paper_train[:, :max_paper_id]\n",
    "\n",
    "# Extract the data\n",
    "paper_ids = paper_c_paper_train_sampled[0].numpy()  # Citing papers\n",
    "cited_ids = paper_c_paper_train_sampled[1].numpy()  # Cited papers\n",
    "\n",
    "# Filter to ensure indices are within range\n",
    "valid_mask = (paper_ids <= max_paper_id) & (cited_ids <= max_paper_id)\n",
    "paper_ids = paper_ids[valid_mask]\n",
    "cited_ids = cited_ids[valid_mask]\n",
    "\n",
    "# Create a sparse matrix in COO format with value 1 for citations\n",
    "citation_values = np.ones(len(paper_ids))  # All citations have value 1\n",
    "citation_matrix_coo = coo_matrix(\n",
    "    (citation_values, (paper_ids, cited_ids)),\n",
    "    shape=(max_paper_id + 1, max_paper_id + 1)\n",
    ")\n",
    "\n",
    "# Convert to dense format to include missing links with label 0\n",
    "full_citation_matrix = citation_matrix_coo.toarray()\n",
    "\n",
    "# Convert back to sparse format for efficiency\n",
    "citation_matrix_coo = coo_matrix(full_citation_matrix)\n",
    "\n",
    "# Use in PyTorch\n",
    "citation_matrix = torch.sparse_coo_tensor(\n",
    "    (citation_matrix_coo.row, citation_matrix_coo.col), citation_matrix_coo.data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, alpha=1.0, eps=1e-8, use_regularization=False, lam=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the loss function with given parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): Scaling parameter for edge probability.\n",
    "            eps (float): Small value to prevent log(0).\n",
    "            use_regularization (bool): Whether to include Gaussian regularization.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.lam = lam\n",
    "        self.use_regularization = use_regularization\n",
    "\n",
    "    def edge_probability(self, z_i, z_j):\n",
    "        \"\"\"Compute the probability of an edge existing between two embeddings.\"\"\"\n",
    "        dist = torch.norm(z_i - z_j) ** 2  # Squared Euclidean distance\n",
    "        exponent = torch.clamp(-self.alpha + dist, min=-50, max=50)  # Prevents extreme values\n",
    "        return 1 / (1 + torch.exp(exponent))  # Logistic function\n",
    "\n",
    "    def link_loss(self, label, z_u, z_v):\n",
    "        \"\"\"Compute the loss for a single edge.\"\"\"\n",
    "        prob = self.edge_probability(z_u, z_v)\n",
    "        prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "        return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)\n",
    "\n",
    "    def compute_loss(self, node_embeddings, datamatrix_tensor):\n",
    "        \"\"\"Compute the total loss for the dataset using sparse tensor.\"\"\"\n",
    "        \n",
    "        # Extract indices and values from the sparse tensor\n",
    "        indices = datamatrix_tensor.coalesce().indices()  # Shape: [2, nnz]\n",
    "        values = datamatrix_tensor.coalesce().values()    # Shape: [nnz]\n",
    "\n",
    "        u_idx = indices[0]  # Citing papers\n",
    "        v_idx = indices[1]  # Cited papers\n",
    "        \n",
    "        # Assume the indices are in the form of (u_idx, v_idx) pairs\n",
    "        z_u = node_embeddings(u_idx)  # Lookup embeddings\n",
    "        z_v = node_embeddings(v_idx)\n",
    "        \n",
    "        # Compute the link loss for the pairs (u_idx, v_idx) using vectorized operations\n",
    "        # self.link_loss should support batch operations for efficiency\n",
    "        losses = self.link_loss(values, z_u, z_v)\n",
    "        # Sum the losses and compute the final loss\n",
    "        sum_loss = losses.sum()  # Sum over all citation events\n",
    "        loss = -sum_loss / len(values)  # Divide by the number of non-zero values (citations)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "# loss_fn = LossFunction(alpha=1.0, use_regularization=True)\n",
    "# loss_value = loss_fn.compute_loss(z, datamatrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_matrix.coalesce().indices().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function\u001b[38;5;241m.\u001b[39mcompute_loss(node_embeddings, citation_matrix)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mLossFunction.compute_loss\u001b[0;34m(self, node_embeddings, datamatrix_tensor)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Assume the indices are in the form of (u_idx, v_idx) pairs\u001b[39;00m\n\u001b[1;32m     41\u001b[0m z_u \u001b[38;5;241m=\u001b[39m node_embeddings(u_idx)  \u001b[38;5;66;03m# Lookup embeddings\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m z_v \u001b[38;5;241m=\u001b[39m node_embeddings(v_idx)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Compute the link loss for the pairs (u_idx, v_idx) using vectorized operations\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# self.link_loss should support batch operations for efficiency\u001b[39;00m\n\u001b[1;32m     46\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink_loss(values, z_u, z_v)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "embedding_dim = 2\n",
    "node_embeddings = torch.nn.Embedding((citation_matrix.coalesce().indices().shape[1]), embedding_dim)\n",
    "\n",
    "\n",
    "loss_function = LossFunction(alpha=5, eps=1e-2, use_regularization=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(node_embeddings.parameters(), lr=0.1)\n",
    "\n",
    "alpha = 5\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function.compute_loss(node_embeddings, citation_matrix)  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Assume paper_c_paper_train is a tensor with shape (2, num_links)\n",
    "tensor = paper_c_paper_train\n",
    "\n",
    "# Extract existing citations\n",
    "paper_ids = tensor[0].numpy()  # Citing papers\n",
    "cited_ids = tensor[1].numpy()  # Cited papers\n",
    "\n",
    "num_papers = max(max(paper_ids), max(cited_ids)) + 1  # Total number of papers\n",
    "\n",
    "# Step 1: Create Positive (Existing) Citations\n",
    "citation_values = np.ones(len(paper_ids))  # All existing citations have value 1\n",
    "citation_matrix_coo = coo_matrix((citation_values, (paper_ids, cited_ids)), shape=(num_papers, num_papers))\n",
    "\n",
    "# Step 2: Generate Negative Samples (Missing Links)\n",
    "num_neg_samples = 100  # Aim for a 1:1 ratio of positive to negative\n",
    "negative_samples = set()\n",
    "\n",
    "while len(negative_samples) < num_neg_samples:\n",
    "    i = np.random.randint(0, num_papers)\n",
    "    j = np.random.randint(0, num_papers)\n",
    "    if i != j and (i, j) not in zip(paper_ids, cited_ids):  # Ensure no duplicates\n",
    "        negative_samples.add((i, j))\n",
    "\n",
    "neg_paper_ids, neg_cited_ids = zip(*negative_samples)\n",
    "neg_values = np.zeros(len(neg_paper_ids))  # Missing links have value 0\n",
    "\n",
    "# Step 3: Merge Positive and Negative Samples\n",
    "all_paper_ids = np.concatenate([paper_ids, np.array(neg_paper_ids)])\n",
    "all_cited_ids = np.concatenate([cited_ids, np.array(neg_cited_ids)])\n",
    "all_values = np.concatenate([citation_values, neg_values])\n",
    "\n",
    "# Create a sparse matrix including both 1s and 0s\n",
    "citation_matrix_coo = coo_matrix((all_values, (all_paper_ids, all_cited_ids)), shape=(num_papers, num_papers))\n",
    "\n",
    "# Step 4: Convert to PyTorch Sparse Tensor\n",
    "indices = torch.tensor([citation_matrix_coo.row, citation_matrix_coo.col], dtype=torch.long)\n",
    "values = torch.tensor(citation_matrix_coo.data, dtype=torch.float32)\n",
    "\n",
    "citation_matrix = torch.sparse_coo_tensor(indices, values, torch.Size([num_papers, num_papers]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    28, 333019])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_c_paper_train[:,88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 7]])\n",
      "tensor(186851)\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(paper_c_paper_train[0] == 2))\n",
    "print(paper_c_paper_train[1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3879968 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3879968/3879968 [00:57<00:00, 67341.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m paper_c_paper_train[\u001b[38;5;241m0\u001b[39m][i] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m     datamatrix\u001b[38;5;241m.\u001b[39mappend([paper_c_paper_train[\u001b[38;5;241m0\u001b[39m][i], paper_c_paper_train[\u001b[38;5;241m1\u001b[39m][i], label])\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(datamatrix)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/_tensor.py:568\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    565\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/_tensor_str.py:704\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    703\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _str_intern(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/_tensor_str.py:583\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    578\u001b[0m         strs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    579\u001b[0m             indented_str(\u001b[38;5;28mstr\u001b[39m(t), indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39munbind\u001b[38;5;241m.\u001b[39mint(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m         tensor_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_is_functional_tensor(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    584\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_to_functional_tensor(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_from_functional_tensor(\u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating datamatrix where 1 if link 0 if not, also add the 1st and 2nd column to the datamatrix\n",
    "from tqdm import tqdm\n",
    "datamatrix = []\n",
    "for i in tqdm(range(len(paper_c_paper_train[0]))):\n",
    "    label = 1 if i == paper_c_paper_train[0][i] else 0\n",
    "    datamatrix.append([paper_c_paper_train[0][i], paper_c_paper_train[1][i], label])\n",
    "\n",
    "print(datamatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
