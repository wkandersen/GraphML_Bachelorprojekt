{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416271, 5416271)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Set working directory\n",
    "\n",
    "try:\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "except FileNotFoundError:\n",
    "    os.chdir(\"..\")\n",
    "    os.chdir(\"..\")\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "# Extract edges for \"paper\" -> \"cites\" -> \"paper\"\n",
    "paper_c_paper = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_c_paper[0], nums_train) | torch.isin(paper_c_paper[1], nums_train)\n",
    "mask_valid = torch.isin(paper_c_paper[0], nums_valid) | torch.isin(paper_c_paper[1], nums_valid)\n",
    "mask_test = torch.isin(paper_c_paper[0], nums_test) | torch.isin(paper_c_paper[1], nums_test)\n",
    "\n",
    "paper_c_paper_train = paper_c_paper.clone()\n",
    "paper_c_paper_valid = paper_c_paper.clone()\n",
    "paper_c_paper_test = paper_c_paper.clone()\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_c_paper_train = paper_c_paper_train[:, mask_train_done]\n",
    "paper_c_paper_valid = paper_c_paper_valid[:, mask_valid_done]\n",
    "paper_c_paper_test = paper_c_paper_test[:, mask_test]\n",
    "\n",
    "len(paper_c_paper_train[1]) + len(paper_c_paper_valid[1]) + len(paper_c_paper_test[1]), paper_c_paper.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    246],\n",
       "        [     1,    131],\n",
       "        [     2,    189],\n",
       "        ...,\n",
       "        [736386,    266],\n",
       "        [736387,    289],\n",
       "        [736388,      1]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y = data['y_dict']['paper']\n",
    "\n",
    "# Get the indices of the tensor\n",
    "indices = torch.arange(tensor_y.size(0)).view(-1, 1)  # Create a tensor of indices\n",
    "\n",
    "# Concatenate the indices with the original tensor\n",
    "tensor_y = torch.cat((indices, tensor_y), dim=1)\n",
    "tensor_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "random.seed(99)\n",
    "torch.manual_seed(99)\n",
    "\n",
    "class mini_batches_code:\n",
    "    def __init__(self,data, unique_list, sample_size,edge_type):\n",
    "        self.data = data\n",
    "        self.sample_size = sample_size\n",
    "        self.edge_type = edge_type\n",
    "        self.unique_list = unique_list\n",
    "\n",
    "    def get_batch(self):\n",
    "        # random.seed(99) \n",
    "        # torch.manual_seed(99)\n",
    "        list_pcp = self.unique_list\n",
    "        random_sample = random.sample(list_pcp, self.sample_size)\n",
    "        print(random_sample)\n",
    "        for value in random_sample:\n",
    "            list_pcp.remove(value)\n",
    "        mask = torch.isin(self.data[0], torch.tensor(random_sample))\n",
    "        filtered_data = self.data[:,mask]\n",
    "        return filtered_data, random_sample, list_pcp\n",
    "    \n",
    "    def data_matrix(self):\n",
    "        data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "        edge_entities = {\n",
    "            'paper': 0,\n",
    "            'author': 1,\n",
    "            'institution': 2,\n",
    "            'field_of_study': 3,\n",
    "            'venue': 4,\n",
    "        }\n",
    "        # Get batch and initialize tensors\n",
    "        tensor, random_sample, unique_list = self.get_batch()\n",
    "\n",
    "        # Create result tensor from input batch\n",
    "        result_tensor = torch.stack([torch.tensor([1, tensor[0, i], tensor[1, i],edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]) for i in range(tensor.shape[1])])\n",
    "\n",
    "        # Initialize lists for non_edges and venues\n",
    "        non_edges, venues = [], []\n",
    "\n",
    "        # Add venue links for sampled nodes\n",
    "        for i in random_sample:\n",
    "            venues.append(torch.tensor([1, i.item(), data['y_dict']['paper'][i], edge_entities[self.edge_type[0]],edge_entities['venue']]))\n",
    "\n",
    "            # Find non-existing edges\n",
    "            for j in tensor[1].unique():\n",
    "                if i != j and not torch.any((result_tensor[:, 1] == i) & (result_tensor[:, 2] == j)): \n",
    "                    non_edges.append(torch.tensor([0, i.item(), j.item(),edge_entities[self.edge_type[0]],edge_entities[self.edge_type[2]]]))\n",
    "\n",
    "        for r, j in itertools.combinations(random_sample, 2):  # itertools generates all unique pairs\n",
    "            if data['y_dict']['paper'][r] != data['y_dict']['paper'][j]:\n",
    "                venues.append(torch.tensor([0, r, data['y_dict']['paper'][j],1,0]))\n",
    "                venues.append(torch.tensor([0, j, data['y_dict']['paper'][r],1,0]))\n",
    "\n",
    "        # Convert lists to tensors only once to optimize memory usage\n",
    "        non_edges_tensor = torch.stack(non_edges) if non_edges else torch.empty((0, 5), dtype=torch.long)\n",
    "        venues_tensor = torch.stack(venues) if venues else torch.empty((0, 5), dtype=torch.long)\n",
    "\n",
    "        # Merge all tensors\n",
    "        data_matrix = torch.cat((result_tensor, non_edges_tensor, venues_tensor), dim=0)\n",
    "        return data_matrix, unique_list\n",
    "\n",
    "\n",
    "# mini_b = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "# dm,l1 = mini_b.data_matrix()\n",
    "# mini_b1 = mini_batches_code(paper_c_paper_train, l1, 10,('paper', 'cites', 'paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, alpha=1.0, eps=1e-8, use_regularization=False):\n",
    "        \"\"\"\n",
    "        Initialize the loss function with given parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): Scaling parameter for edge probability.\n",
    "            eps (float): Small value to prevent log(0).\n",
    "            use_regularization (bool): Whether to include Gaussian regularization.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.use_regularization = use_regularization\n",
    "\n",
    "    # def edge_probability(self, z_i, z_j):\n",
    "    #     \"\"\"Compute the probability of an edge existing between two embeddings.\"\"\"\n",
    "    #     dist = torch.norm(z_i - z_j) ** 2  # Squared Euclidean distance\n",
    "    #     return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    # def link_loss(self, label, z_u, z_v):\n",
    "    #     \"\"\"Compute the loss for a single edge.\"\"\"\n",
    "    #     prob = self.edge_probability(z_u, z_v)\n",
    "    #     prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "    #     return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)        \n",
    "\n",
    "    # def compute_loss(self, z, datamatrix_tensor):\n",
    "    #     \"\"\"Compute the total loss for the dataset.\"\"\"\n",
    "    #     sum_loss = sum(\n",
    "    #         self.link_loss(label, z[u_idx], z[v_idx])\n",
    "    #         for label, u_idx, v_idx in datamatrix_tensor\n",
    "    #     )\n",
    "\n",
    "    #     loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "    #     if self.use_regularization:\n",
    "    #         regularization = -0.5 * torch.sum(z ** 2)\n",
    "    #         loss += regularization\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "\n",
    "\n",
    "    def edge_probability(self, z_i, z_j, type_i, type_j):\n",
    "        \"\"\"Compute the probability of an edge existing between two nodes, considering embeddings and types.\"\"\"\n",
    "        # Convert types to one-hot encoded vectors\n",
    "        type_i = type_i.view(1, -1).float()\n",
    "        type_j = type_j.view(1, -1).float()\n",
    "        # Ensure types are float tensors for concatenation\n",
    "\n",
    "        # Combine the node embeddings and types (one-hot encoded)\n",
    "        z_i = z_i.view(1, -1).float()  # Ensure z_i is a float tensor\n",
    "        z_j = z_j.view(1, -1).float()  # Ensure z_j is a float tensor\n",
    "        \n",
    "        combined_i = torch.cat((z_i, type_i), dim=-1)  # Concatenate embedding and type for node i\n",
    "        combined_j = torch.cat((z_j, type_j), dim=-1)  # Concatenate embedding and type for node j\n",
    "        \n",
    "        dist = torch.norm(combined_i - combined_j) ** 2  # Squared Euclidean distance\n",
    "        return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    def link_loss(self, label, z_u, z_v, type_u, type_v):\n",
    "        \"\"\"Compute the loss for a single edge, considering node types.\"\"\"\n",
    "        prob = self.edge_probability(z_u, z_v, type_u, type_v)\n",
    "        prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "        return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)\n",
    "\n",
    "    def compute_loss(self, z, types, datamatrix_tensor):\n",
    "        \"\"\"Compute the total loss for the dataset, considering node types.\"\"\"\n",
    "        sum_loss = sum(\n",
    "            self.link_loss(label, z[idx][0], z[idx][1], types[idx][0], types[idx][1])\n",
    "            for idx, (label, _, _) in enumerate(datamatrix_tensor)\n",
    "        )\n",
    "\n",
    "        loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "        if self.use_regularization:\n",
    "            regularization = self.lam * torch.sum(z ** 2)\n",
    "            loss += regularization\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# loss_fn = LossFunction(alpha=1.0, use_regularization=True)\n",
    "# loss_value = loss_fn.compute_loss(z, datamatrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(423601), np.int64(399248), np.int64(209794), np.int64(628557), np.int64(187487), np.int64(241447), np.int64(260499), np.int64(139716), np.int64(90784), np.int64(263350)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_b = mini_batches_code(paper_c_paper_train, list(paper_c_paper.unique().numpy()), 10,('paper', 'cites', 'paper'))\n",
    "dm,l1 = mini_b.data_matrix()\n",
    "dm[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.0530\n",
      "Epoch 10: Loss = 0.9286\n",
      "Epoch 20: Loss = 0.8058\n",
      "Epoch 30: Loss = 0.6884\n",
      "Epoch 40: Loss = 0.5831\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "datamatrix_tensor = dm\n",
    "num_nodes = dm.shape[0]\n",
    "# 2️ Define Embeddings\n",
    "embedding_dim = 2\n",
    "node_embeddings = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "optimizer = torch.optim.Adam(node_embeddings.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=False)\n",
    "\n",
    "# 3️ Train Embeddings\n",
    "alpha = 3\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z = node_embeddings.weight  # Get embeddings\n",
    "    types = dm[:,3:]\n",
    "    loss = loss_function.compute_loss(z, types, datamatrix_tensor[:,:3])  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starter på dataloader her"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
