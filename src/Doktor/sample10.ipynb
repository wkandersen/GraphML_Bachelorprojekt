{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5416271, 5416271)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Set working directory\n",
    "\n",
    "try:\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "except FileNotFoundError:\n",
    "    os.chdir(\"..\")\n",
    "    os.chdir(\"..\")\n",
    "    data_train = pd.read_csv('dataset/ogbn_mag/split/time/paper/train.csv.gz', compression='gzip',header = None)\n",
    "    data_valid = pd.read_csv('dataset/ogbn_mag/split/time/paper/valid.csv.gz', compression='gzip',header = None)\n",
    "    data_test = pd.read_csv('dataset/ogbn_mag/split/time/paper/test.csv.gz', compression='gzip',header = None)\n",
    "\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "# Extract edges for \"paper\" -> \"cites\" -> \"paper\"\n",
    "paper_c_paper = data.edge_index_dict[('paper', 'cites', 'paper')]\n",
    "\n",
    "# Unique paper IDs to keep (Ensure it's a PyTorch tensor)\n",
    "nums_valid = torch.tensor(data_valid[0])\n",
    "nums_test = torch.tensor(data_test[0])\n",
    "nums_train = torch.tensor(data_train[0])\n",
    "\n",
    "mask_train = torch.isin(paper_c_paper[0], nums_train) | torch.isin(paper_c_paper[1], nums_train)\n",
    "mask_valid = torch.isin(paper_c_paper[0], nums_valid) | torch.isin(paper_c_paper[1], nums_valid)\n",
    "mask_test = torch.isin(paper_c_paper[0], nums_test) | torch.isin(paper_c_paper[1], nums_test)\n",
    "\n",
    "paper_c_paper_train = paper_c_paper.clone()\n",
    "paper_c_paper_valid = paper_c_paper.clone()\n",
    "paper_c_paper_test = paper_c_paper.clone()\n",
    "\n",
    "# Combine the conditions into a single mask that selects only the train edges\n",
    "mask_train_done = mask_train & ~mask_valid & ~mask_test\n",
    "mask_valid_done = mask_valid & ~mask_test\n",
    "\n",
    "# Apply the combined mask to paper_c_paper_train\n",
    "paper_c_paper_train = paper_c_paper_train[:, mask_train_done]\n",
    "paper_c_paper_valid = paper_c_paper_valid[:, mask_valid_done]\n",
    "paper_c_paper_test = paper_c_paper_test[:, mask_test]\n",
    "\n",
    "len(paper_c_paper_train[1]) + len(paper_c_paper_valid[1]) + len(paper_c_paper_test[1]), paper_c_paper.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    246],\n",
       "        [     1,    131],\n",
       "        [     2,    189],\n",
       "        ...,\n",
       "        [736386,    266],\n",
       "        [736387,    289],\n",
       "        [736388,      1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y = data['y_dict']['paper']\n",
    "\n",
    "# Get the indices of the tensor\n",
    "indices = torch.arange(tensor_y.size(0)).view(-1, 1)  # Create a tensor of indices\n",
    "\n",
    "# Concatenate the indices with the original tensor\n",
    "tensor_y = torch.cat((indices, tensor_y), dim=1)\n",
    "tensor_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85053/3727153333.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  citation_matrix = torch.sparse_coo_tensor((citation_matrix_coo.row, citation_matrix_coo.col), citation_matrix_coo.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[     0,      0,      0,  ..., 736388, 736388, 736388],\n",
       "                       [    88,  27449, 121051,  ..., 421711, 427339, 439864]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(736389, 736389), nnz=3879968, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "\n",
    "# Example citation data: Replace with your actual data\n",
    "# For example:\n",
    "# paper_ids = [0, 88, 27449, 121051, ...]  # Citing papers\n",
    "# cited_ids = [88, 27449, 121051, ..., 421711, 427339, 439864]  # Cited papers\n",
    "\n",
    "# Example tensor as you mentioned (2 rows, multiple columns)\n",
    "# tensor[0] represents paper_ids (papers that cite others)\n",
    "# tensor[1] represents cited paper_ids (papers being cited)\n",
    "tensor = paper_c_paper_train \n",
    "\n",
    "# Extract the data\n",
    "paper_ids = tensor[0].numpy()  # Citing papers\n",
    "cited_ids = tensor[1].numpy()  # Cited papers\n",
    "\n",
    "# Create a sparse matrix in COO format:\n",
    "citation_values = np.ones(len(paper_ids))  # All citations will have value 1\n",
    "\n",
    "# Create the sparse matrix with the correct shape\n",
    "citation_matrix_coo = coo_matrix((citation_values, (paper_ids, cited_ids)), shape=(max(paper_ids) + 1, max(cited_ids) + 1))\n",
    "\n",
    "# Now you have a sparse matrix with 3 columns (paper_id, cited_paper_id, 1 for citation)\n",
    "# Optionally, you can convert it to CSR format for better performance:\n",
    "citation_matrix_csr = citation_matrix_coo.tocsr()\n",
    "\n",
    "# If you want to save this matrix to disk (e.g., in HDF5 format), you can do so:\n",
    "# import h5py\n",
    "# with h5py.File('citation_matrix.h5', 'w') as f:\n",
    "#     f.create_dataset('rows', data=citation_matrix_coo.row)\n",
    "#     f.create_dataset('cols', data=citation_matrix_coo.col)\n",
    "#     f.create_dataset('data', data=citation_matrix_coo.data)\n",
    "\n",
    "\n",
    "# use in pytorch\n",
    "citation_matrix = torch.sparse_coo_tensor((citation_matrix_coo.row, citation_matrix_coo.col), citation_matrix_coo.data)\n",
    "citation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([[     0,      0,      0,  ...,     32,     32,     32],\n",
       "                        [    61,     88,    115,  ..., 584314, 610792, 721561]]),\n",
       "        values=tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       "        size=(33, 721562), nnz=2300, dtype=torch.float64, layout=torch.sparse_coo),\n",
       " tensor([[     0,      0,     61],\n",
       "         [     1,      0,     88],\n",
       "         [     0,      0,    115],\n",
       "         ...,\n",
       "         [     0,     32, 584314],\n",
       "         [     0,     32, 610792],\n",
       "         [     0,     32, 721561]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the sparse tensor is coalesced\n",
    "citation_matrix = citation_matrix.coalesce()\n",
    "\n",
    "# Extract first 10 indices and values\n",
    "indices = citation_matrix.indices()[:, :100]  \n",
    "values = citation_matrix.values()[:100]       \n",
    "\n",
    "new_indices = []\n",
    "new_values = []\n",
    "\n",
    "# Check for missing entries\n",
    "for i in indices[0].unique():\n",
    "    for j in indices[1].unique():\n",
    "        if citation_matrix[i, j] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            new_indices.append(torch.tensor([[i], [j]]))  # Store new index\n",
    "            new_values.append(torch.tensor([0]))  # Store new value\n",
    "\n",
    "# If new indices exist, add them all at once\n",
    "if new_indices:\n",
    "    new_indices = torch.cat(new_indices, dim=1)\n",
    "    new_values = torch.cat(new_values, dim=0)\n",
    "\n",
    "    # Append to existing data\n",
    "    updated_indices = torch.cat((indices, new_indices), dim=1)\n",
    "    updated_values = torch.cat((values, new_values), dim=0)\n",
    "\n",
    "    # Create new sparse tensor\n",
    "    citation_matrix = torch.sparse_coo_tensor(updated_indices, updated_values)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ensure the sparse tensor is coalesced\n",
    "citation_matrix = citation_matrix.coalesce()\n",
    "\n",
    "# Extract indices and values\n",
    "indices = citation_matrix.indices()  # Shape: (2, nnz) where nnz = number of nonzero elements\n",
    "values = citation_matrix.values()    # Shape: (nnz,)\n",
    "\n",
    "# Stack values, row indices, and column indices into a single array\n",
    "tensor_array = torch.stack((values, indices[0], indices[1]), dim=1)  # Shape: (nnz, 3)\n",
    "tensor_array = tensor_array.to(torch.long)\n",
    "# Print the transformed tensor\n",
    "print(len(tensor_array))\n",
    "all_nodes = torch.cat((indices[0], indices[1]), dim=0).unique()\n",
    "nodes_set_1 = indices[0].unique()\n",
    "nodes_set_2 = indices[1].unique()\n",
    "max_node_id = torch.max(indices[1])  # Assuming the second column of `indices` represents node IDs\n",
    "num_nodes = max_node_id + 1\n",
    "\n",
    "citation_matrix,tensor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, alpha=1.0, eps=1e-8, use_regularization=False):\n",
    "        \"\"\"\n",
    "        Initialize the loss function with given parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): Scaling parameter for edge probability.\n",
    "            eps (float): Small value to prevent log(0).\n",
    "            use_regularization (bool): Whether to include Gaussian regularization.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.use_regularization = use_regularization\n",
    "\n",
    "    def edge_probability(self, z_i, z_j):\n",
    "        \"\"\"Compute the probability of an edge existing between two embeddings.\"\"\"\n",
    "        dist = torch.norm(z_i - z_j) ** 2  # Squared Euclidean distance\n",
    "        return 1 / (1 + torch.exp(-self.alpha + dist))  # Logistic function\n",
    "\n",
    "    def link_loss(self, label, z_u, z_v):\n",
    "        \"\"\"Compute the loss for a single edge.\"\"\"\n",
    "        prob = self.edge_probability(z_u, z_v)\n",
    "        prob = torch.clamp(prob, self.eps, 1 - self.eps)  # Numerical stability\n",
    "\n",
    "        return label.float() * torch.log(prob) + (1 - label.float()) * torch.log(1 - prob)        \n",
    "\n",
    "    def compute_loss(self, z, datamatrix_tensor):\n",
    "        \"\"\"Compute the total loss for the dataset.\"\"\"\n",
    "        sum_loss = sum(\n",
    "            self.link_loss(label, z[u_idx], z[v_idx])\n",
    "            for label, u_idx, v_idx in datamatrix_tensor\n",
    "        )\n",
    "\n",
    "        loss = -sum_loss / len(datamatrix_tensor)\n",
    "\n",
    "        if self.use_regularization:\n",
    "            regularization = -0.5 * torch.sum(z ** 2)\n",
    "            loss += regularization\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# loss_fn = LossFunction(alpha=1.0, use_regularization=True)\n",
    "# loss_value = loss_fn.compute_loss(z, datamatrix_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.4476\n",
      "Epoch 1: Loss = 0.4441\n",
      "Epoch 2: Loss = 0.4407\n",
      "Epoch 3: Loss = 0.4373\n",
      "Epoch 4: Loss = 0.4339\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "datamatrix_tensor = tensor_array\n",
    "\n",
    "# 2️ Define Embeddings\n",
    "embedding_dim = 2\n",
    "node_embeddings = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "optimizer = torch.optim.Adam(node_embeddings.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = LossFunction(alpha=1.0, eps=1e-10, use_regularization=False)\n",
    "\n",
    "# 3️ Train Embeddings\n",
    "alpha = 3\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    z = node_embeddings.weight  # Get embeddings\n",
    "    loss = loss_function.compute_loss(z, datamatrix_tensor)  # Compute loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starter på dataloader her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[662957]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(2, 0), dtype=torch.int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "paper_c_paper_train[0],paper_c_paper_train[1]\n",
    "list_pcp = list(paper_c_paper.unique().numpy())\n",
    "random_sample = random.sample(list_pcp, 1)\n",
    "print(random_sample)\n",
    "\n",
    "for value in random_sample:\n",
    "    list_pcp.remove(value)\n",
    "\n",
    "mask = torch.isin(paper_c_paper_train[0], torch.tensor(random_sample))\n",
    "\n",
    "# Use the mask to select the rows that match the random sample\n",
    "filtered_data = paper_c_paper_train[:,mask]\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(99) \n",
    "torch.manual_seed(99)\n",
    "class mini_batches_code:\n",
    "    def __init__(self,data,sample_size):\n",
    "        self.data = data\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def get_batch(self):\n",
    "        random.seed(99) \n",
    "        torch.manual_seed(99)\n",
    "        list_pcp = list(self.data[0].unique().numpy())\n",
    "        random_sample = random.sample(list_pcp, self.sample_size)\n",
    "        print(random_sample)\n",
    "        for value in random_sample:\n",
    "            list_pcp.remove(value)\n",
    "        mask = torch.isin(self.data[0], torch.tensor(random_sample))\n",
    "        filtered_data = self.data[:,mask]\n",
    "        return filtered_data, random_sample\n",
    "    \n",
    "    def data_matrix(self):\n",
    "        tensor, random_sample = self.get_batch()\n",
    "        new_arrays = []\n",
    "\n",
    "        for i in range(tensor.shape[1]):\n",
    "            # For each column, create an array starting with 1 and followed by the values from the column\n",
    "            new_array = torch.tensor([1, tensor[0, i], tensor[1, i]])\n",
    "            new_arrays.append(new_array)\n",
    "\n",
    "        # Stack the arrays to form the final tensor\n",
    "        result_tensor = torch.stack(new_arrays)\n",
    "\n",
    "        non_edges = []\n",
    "\n",
    "        for i in random_sample:\n",
    "            for j in tensor[1].unique():\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if not (torch.any((result_tensor[:, 1] == i) & (result_tensor[:, 2] == j))): # Check if the edge exists\n",
    "                    non_edges.append(torch.tensor([0, i.item(), j.item()]))\n",
    "\n",
    "        non_edges_tensor = torch.stack(non_edges)\n",
    "        data_matrix = torch.cat((result_tensor, non_edges_tensor), dim=0)\n",
    "        return data_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     1,      2, 186851],\n",
       "        [     1,      2, 376347],\n",
       "        [     1,      2, 410167],\n",
       "        [     1,      5,  13032],\n",
       "        [     0,      5, 186851],\n",
       "        [     0,      5, 376347],\n",
       "        [     0,      5, 410167],\n",
       "        [     0,      2,  13032]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_b = mini_batches_code(paper_c_paper_train[:,:10], 2)\n",
    "mini_b.data_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 0]\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([189])\n",
      "tensor([189])\n",
      "tensor([189])\n",
      "tensor([189])\n",
      "tensor([189])\n",
      "tensor([189])\n",
      "tensor([246])\n",
      "tensor([246])\n",
      "tensor([246])\n",
      "tensor([246])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     1,      0,     88],\n",
       "        [     1,      0,  27449],\n",
       "        [     1,      0, 121051],\n",
       "        [     1,      0, 151667],\n",
       "        [     1,      0, 308499],\n",
       "        [     1,      2, 186851],\n",
       "        [     1,      2, 376347],\n",
       "        [     1,      2, 410167],\n",
       "        [     1,      5,  13032],\n",
       "        [     0,      5,     88],\n",
       "        [     0,      5,  27449],\n",
       "        [     0,      5, 121051],\n",
       "        [     0,      5, 151667],\n",
       "        [     0,      5, 186851],\n",
       "        [     0,      5, 308499],\n",
       "        [     0,      5, 376347],\n",
       "        [     0,      5, 410167],\n",
       "        [     0,      2,     88],\n",
       "        [     0,      2,  13032],\n",
       "        [     0,      2,  27449],\n",
       "        [     0,      2, 121051],\n",
       "        [     0,      2, 151667],\n",
       "        [     0,      2, 308499],\n",
       "        [     0,      0,  13032],\n",
       "        [     0,      0, 186851],\n",
       "        [     0,      0, 376347],\n",
       "        [     0,      0, 410167],\n",
       "        [     1,      5,      9],\n",
       "        [     1,      2,    189],\n",
       "        [     1,      0,    246],\n",
       "        [     0,      5,    189],\n",
       "        [     0,      2,    246]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mini_b = mini_batches_code(paper_c_paper_train[:,:10], 3)\n",
    "\n",
    "# Original tensor\n",
    "tensor, random_sample = mini_b.get_batch()\n",
    "\n",
    "# Initialize an empty list to collect the new arrays\n",
    "new_arrays = []\n",
    "\n",
    "# Iterate over each column of the tensor\n",
    "for i in range(tensor.shape[1]):\n",
    "    # For each column, create an array starting with 1 and followed by the values from the column\n",
    "    new_array = torch.tensor([1, tensor[0, i], tensor[1, i]])\n",
    "    new_arrays.append(new_array)\n",
    "\n",
    "# Stack the arrays to form the final tensor\n",
    "result_tensor = torch.stack(new_arrays)\n",
    "\n",
    "non_edges = []\n",
    "venues = []\n",
    "\n",
    "for i in random_sample:\n",
    "    venues.append(torch.tensor([1,i.item(),data['y_dict']['paper'][i]]))\n",
    "    for j in tensor[1].unique():\n",
    "        if i == j:\n",
    "            continue\n",
    "        if not (torch.any((result_tensor[:, 1] == i) & (result_tensor[:, 2] == j))): # Check if the edge exists\n",
    "            non_edges.append(torch.tensor([0, i.item(), j.item()]))\n",
    "            print(data['y_dict']['paper'][i])\n",
    "\n",
    "for idx,r in enumerate(random_sample[:-1]):\n",
    "    if data['y_dict']['paper'][r] != data['y_dict']['paper'][random_sample[idx+1]]:\n",
    "        venues.append(torch.tensor([0,r.item(),data['y_dict']['paper'][random_sample[idx+1]]]))\n",
    "\n",
    "non_edges_tensor = torch.stack(non_edges)\n",
    "venues_tensor = torch.stack(venues)\n",
    "data_matrix_1 = torch.cat((result_tensor, non_edges_tensor), dim=0)\n",
    "data_matrix = torch.cat((data_matrix_1, venues_tensor), dim=0)\n",
    "data_matrix\n",
    "\n",
    "# tensor_y = data['y_dict']['paper'][i]\n",
    "\n",
    "# # Get the indices of the tensor\n",
    "# indices = torch.arange(tensor_y.size(0)).view(-1, 1)  # Create a tensor of indices\n",
    "\n",
    "# # Concatenate the indices with the original tensor\n",
    "# tensor_y = torch.cat((indices, tensor_y), dim=1)\n",
    "# tensor_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    246],\n",
       "        [     1,    131],\n",
       "        [     2,    189],\n",
       "        ...,\n",
       "        [736386,    266],\n",
       "        [736387,    289],\n",
       "        [736388,      1]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
