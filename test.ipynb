{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71534b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example forward and backward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m embedded \u001b[38;5;241m=\u001b[39m embedding(input_ids)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Assume some loss computed on the full 136-dimensional vectors\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/Bachelorprojekt/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example embedding: vocab size 1000, embedding dim 136\n",
    "embedding = nn.Embedding(1, 136)\n",
    "\n",
    "# Masking function for gradient hook\n",
    "def mask_gradients(grad):\n",
    "    # grad shape: [vocab_size, 136]\n",
    "    mask = torch.zeros_like(grad)\n",
    "    mask[:, :8] = 1  # only keep gradients for first 8 dims\n",
    "    return grad * mask\n",
    "\n",
    "# Register the hook\n",
    "embedding.weight.register_hook(mask_gradients)\n",
    "\n",
    "# Example forward and backward pass\n",
    "\n",
    "embedded = embedding(input_ids)\n",
    "\n",
    "# Assume some loss computed on the full 136-dimensional vectors\n",
    "loss = embedded.pow(2).sum()\n",
    "loss.backward()\n",
    "\n",
    "# optimizer.step() would now only update the first 8 dims\n",
    "# Print the gradients to verify\n",
    "print(embedding.weight.grad)  # Should show gradients only for the first 8 dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf47cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "from Packages.mini_batches import mini_batches_code\n",
    "from Packages.loss_function import LossFunction\n",
    "from Packages.data_divide import paper_c_paper_train, paper_c_paper_valid\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "emb_matrix = torch.load(\"dataset/ogbn_mag/processed/hpc/emb_matrix_8_125_epoch.pt\", map_location=device)\n",
    "# paper_c_paper_valid = torch.load(\"dataset/ogbn_mag/processed/paper_c_paper_valid.pt\", map_location=device)\n",
    "data, _ = torch.load(r\"dataset/ogbn_mag/processed/geometric_data_processed.pt\", weights_only=False)\n",
    "\n",
    "valid_dict = {}\n",
    "\n",
    "# Get unique node IDs from both train and valid edges\n",
    "unique_train = set(paper_c_paper_train.flatten().unique().tolist())\n",
    "unique_valid = set(paper_c_paper_valid.flatten().unique().tolist())\n",
    "\n",
    "# Keep only validation nodes that do not appear in training edges\n",
    "valid_exclusive = unique_valid - unique_train\n",
    "\n",
    "# Initial list of nodes for iterations\n",
    "l_prev = list(valid_exclusive)\n",
    "num_iterations = int(len(l_prev)-1)\n",
    "\n",
    "sample = 1\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "\n",
    "    # Generate mini-batches\n",
    "    mini_b_new = mini_batches_code(paper_c_paper_valid, l_prev, sample, ('paper', 'cites', 'paper'),data)\n",
    "    dm_new,l_next,remapped_datamatrix_tensor_new,random_sample = mini_b_new.node_mapping()\n",
    "\n",
    "    dm_new = dm_new.to(device)\n",
    "    remapped_datamatrix_tensor_new = remapped_datamatrix_tensor_new.to(device)\n",
    "\n",
    "    new_datamatrix = dm_new[torch.all(dm_new[:, 4:] != 4, dim=1)]\n",
    "    new_remapped_datamatrix_tensor_new = remapped_datamatrix_tensor_new[torch.all(remapped_datamatrix_tensor_new[:, 4:] != 4, dim=1)]\n",
    "\n",
    "    loss_function = LossFunction(alpha=10, eps=1e-10, use_regularization=True, lam=0.001)\n",
    "    for j in range(sample):\n",
    "\n",
    "        new_embedding = torch.nn.Embedding(sample, 8).to(device)\n",
    "        valid_dict[random_sample[j]] = new_embedding\n",
    "        new_embedding.weight = torch.mean(emb_matrix[dm_new[:, 0] == 1], dim=0 and dm_new[:, 1] == random_sample[j]).unsqueeze(0)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    new_optimizer = torch.optim.Adam(new_embedding.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 30\n",
    "    \n",
    "        # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        new_optimizer.zero_grad()\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        temp_embed = torch.cat([emb_matrix, new_embedding.weight], dim=0)\n",
    "        types = new_datamatrix[:, 3:]\n",
    "        loss = loss_function.compute_loss(temp_embed, new_remapped_datamatrix_tensor_new[:, :3])  # Compute loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        new_optimizer.step()\n",
    "\n",
    "        # Print loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Update node list for the next iteration\n",
    "    l_prev = l_next\n",
    "\n",
    "    # valid_dict[random_sample[0]] = new_embedding.weight.detach().cpu().clone()\n",
    "\n",
    "    # Cleanup\n",
    "    if (i + 1) % 10 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(valid_dict, \"dataset/ogbn_mag/processed/hpc/valid_dict_8.pt\")\n",
    "\n",
    "print('embed_valid done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464488d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([-1.2961, -1.0552], requires_grad=True),\n",
       " 1: tensor([-0.5786, -0.5141], requires_grad=True),\n",
       " 2: tensor([-0.4999,  1.4018], requires_grad=True),\n",
       " 3: tensor([0.5730, 0.5671], requires_grad=True),\n",
       " 4: tensor([ 1.1350, -0.0300], requires_grad=True),\n",
       " 5: tensor([ 1.7031, -0.8159], requires_grad=True),\n",
       " 6: tensor([ 0.6272, -0.6556], requires_grad=True),\n",
       " 7: tensor([0.3111, 0.0467], requires_grad=True),\n",
       " 8: tensor([ 1.3843, -1.2562], requires_grad=True),\n",
       " 9: tensor([0.1344, 0.5471], requires_grad=True),\n",
       " 10: tensor([1.0425, 1.1302], requires_grad=True),\n",
       " 11: tensor([ 0.0474, -0.1102], requires_grad=True),\n",
       " 12: tensor([ 1.6026, -0.9757], requires_grad=True),\n",
       " 13: tensor([-0.4701, -0.8018], requires_grad=True),\n",
       " 14: tensor([0.2122, 0.1687], requires_grad=True),\n",
       " 15: tensor([1.0261, 0.6671], requires_grad=True),\n",
       " 16: tensor([-0.8706, -0.6984], requires_grad=True),\n",
       " 17: tensor([ 0.5884, -1.9676], requires_grad=True),\n",
       " 18: tensor([ 0.7311, -0.7635], requires_grad=True),\n",
       " 19: tensor([0.3876, 0.6629], requires_grad=True),\n",
       " 20: tensor([-0.7027,  0.7234], requires_grad=True),\n",
       " 21: tensor([ 0.0600, -0.2805], requires_grad=True),\n",
       " 22: tensor([-0.7888, -0.3621], requires_grad=True),\n",
       " 23: tensor([0.2191, 0.9064], requires_grad=True),\n",
       " 24: tensor([ 1.1986, -0.2804], requires_grad=True),\n",
       " 25: tensor([-1.1450, -1.1643], requires_grad=True),\n",
       " 26: tensor([0.5850, 0.7948], requires_grad=True),\n",
       " 27: tensor([-0.7799, -0.9828], requires_grad=True),\n",
       " 28: tensor([-1.9395,  0.2261], requires_grad=True),\n",
       " 29: tensor([ 0.2156, -1.3395], requires_grad=True),\n",
       " 30: tensor([-0.4421,  0.6226], requires_grad=True),\n",
       " 31: tensor([-1.1849, -0.1364], requires_grad=True),\n",
       " 32: tensor([-0.6796,  0.2076], requires_grad=True),\n",
       " 33: tensor([-0.4773,  0.3306], requires_grad=True),\n",
       " 34: tensor([0.4511, 0.6425], requires_grad=True),\n",
       " 35: tensor([-1.5150, -1.2565], requires_grad=True),\n",
       " 36: tensor([ 2.5580, -2.5173], requires_grad=True),\n",
       " 37: tensor([-1.4109, -2.0076], requires_grad=True),\n",
       " 38: tensor([-0.3806, -0.4589], requires_grad=True),\n",
       " 39: tensor([-0.5442,  0.7827], requires_grad=True),\n",
       " 40: tensor([-0.3532,  0.0969], requires_grad=True),\n",
       " 41: tensor([-2.2588,  0.3505], requires_grad=True),\n",
       " 42: tensor([-0.7035, -1.4375], requires_grad=True),\n",
       " 43: tensor([ 1.2868, -0.4216], requires_grad=True),\n",
       " 44: tensor([ 0.9171, -0.2208], requires_grad=True),\n",
       " 45: tensor([-1.3221, -0.8781], requires_grad=True),\n",
       " 46: tensor([1.1058, 0.3540], requires_grad=True),\n",
       " 47: tensor([0.5054, 0.2082], requires_grad=True),\n",
       " 48: tensor([ 0.2145, -0.3653], requires_grad=True),\n",
       " 49: tensor([ 0.6088, -1.8581], requires_grad=True),\n",
       " 50: tensor([-0.5557, -1.4197], requires_grad=True),\n",
       " 51: tensor([-0.5509, -1.1511], requires_grad=True),\n",
       " 52: tensor([2.0407, 0.5030], requires_grad=True),\n",
       " 53: tensor([-1.0001,  1.4557], requires_grad=True),\n",
       " 54: tensor([0.1719, 0.0500], requires_grad=True),\n",
       " 55: tensor([-0.8245,  1.1790], requires_grad=True),\n",
       " 56: tensor([-0.3716, -0.0896], requires_grad=True),\n",
       " 57: tensor([-0.0446,  1.0695], requires_grad=True),\n",
       " 58: tensor([-0.4738, -0.6767], requires_grad=True),\n",
       " 59: tensor([1.3929, 0.2976], requires_grad=True),\n",
       " 60: tensor([-0.0268, -1.7478], requires_grad=True),\n",
       " 61: tensor([ 0.4823, -0.6752], requires_grad=True),\n",
       " 62: tensor([1.8910, 0.0831], requires_grad=True),\n",
       " 63: tensor([ 2.5284, -0.7668], requires_grad=True),\n",
       " 64: tensor([1.5182, 0.8778], requires_grad=True),\n",
       " 65: tensor([0.1997, 0.0731], requires_grad=True),\n",
       " 66: tensor([-0.3979, -0.9798], requires_grad=True),\n",
       " 67: tensor([-0.4546,  0.3875], requires_grad=True),\n",
       " 68: tensor([-0.2880, -0.9308], requires_grad=True),\n",
       " 69: tensor([ 0.8969, -1.2624], requires_grad=True),\n",
       " 70: tensor([0.1912, 0.1951], requires_grad=True),\n",
       " 71: tensor([-0.1304, -1.4412], requires_grad=True),\n",
       " 72: tensor([-0.6315, -1.0542], requires_grad=True),\n",
       " 73: tensor([-0.0677, -0.0003], requires_grad=True),\n",
       " 74: tensor([ 1.2818, -0.2593], requires_grad=True),\n",
       " 75: tensor([ 0.6373, -0.4933], requires_grad=True),\n",
       " 76: tensor([-1.3242, -0.8709], requires_grad=True),\n",
       " 77: tensor([-1.6143,  0.6187], requires_grad=True),\n",
       " 78: tensor([-0.6952, -0.8248], requires_grad=True),\n",
       " 79: tensor([-0.5114, -1.3674], requires_grad=True),\n",
       " 80: tensor([ 1.3413, -1.9378], requires_grad=True),\n",
       " 81: tensor([-0.0917, -0.8618], requires_grad=True),\n",
       " 82: tensor([-1.4990,  1.5611], requires_grad=True),\n",
       " 83: tensor([ 0.3465, -0.9050], requires_grad=True),\n",
       " 84: tensor([-0.2686, -0.6840], requires_grad=True),\n",
       " 85: tensor([ 1.2534, -0.2723], requires_grad=True),\n",
       " 86: tensor([ 0.8273, -0.9371], requires_grad=True),\n",
       " 87: tensor([-0.8031, -0.5347], requires_grad=True),\n",
       " 88: tensor([-0.3295, -0.1637], requires_grad=True),\n",
       " 89: tensor([-0.2191, -0.8094], requires_grad=True),\n",
       " 90: tensor([1.9423, 2.9839], requires_grad=True),\n",
       " 91: tensor([-0.9920,  0.0519], requires_grad=True),\n",
       " 92: tensor([0.1171, 0.5782], requires_grad=True),\n",
       " 93: tensor([0.5502, 0.1804], requires_grad=True),\n",
       " 94: tensor([-1.5482,  0.1681], requires_grad=True),\n",
       " 95: tensor([-0.8259,  0.6782], requires_grad=True),\n",
       " 96: tensor([-0.2228, -0.4879], requires_grad=True),\n",
       " 97: tensor([-0.1013, -1.1867], requires_grad=True),\n",
       " 98: tensor([-0.0443, -1.4211], requires_grad=True),\n",
       " 99: tensor([-0.7431,  1.1479], requires_grad=True),\n",
       " 100: tensor([ 1.3671, -0.8535], requires_grad=True),\n",
       " 101: tensor([0.1250, 1.1924], requires_grad=True),\n",
       " 102: tensor([ 2.7304, -1.1805], requires_grad=True),\n",
       " 103: tensor([-0.2546, -1.4603], requires_grad=True),\n",
       " 104: tensor([0.0922, 0.4482], requires_grad=True),\n",
       " 105: tensor([-0.9223,  1.2162], requires_grad=True),\n",
       " 106: tensor([ 0.2555, -0.0759], requires_grad=True),\n",
       " 107: tensor([-1.9156,  0.1877], requires_grad=True),\n",
       " 108: tensor([-0.7672,  0.7827], requires_grad=True),\n",
       " 109: tensor([-1.1069,  1.0740], requires_grad=True),\n",
       " 110: tensor([-1.4172,  0.0769], requires_grad=True),\n",
       " 111: tensor([0.2824, 0.2335], requires_grad=True),\n",
       " 112: tensor([0.9892, 0.2417], requires_grad=True),\n",
       " 113: tensor([ 0.1615, -0.9846], requires_grad=True),\n",
       " 114: tensor([-0.8547, -0.4033], requires_grad=True),\n",
       " 115: tensor([ 1.3164, -0.5960], requires_grad=True),\n",
       " 116: tensor([-0.5781, -0.8780], requires_grad=True),\n",
       " 117: tensor([1.0302, 0.9368], requires_grad=True),\n",
       " 118: tensor([0.0725, 0.5192], requires_grad=True),\n",
       " 119: tensor([ 0.5880, -1.5639], requires_grad=True),\n",
       " 120: tensor([-1.2926, -1.1233], requires_grad=True),\n",
       " 121: tensor([ 0.0909, -0.7083], requires_grad=True),\n",
       " 122: tensor([-1.7769, -1.2829], requires_grad=True),\n",
       " 123: tensor([-0.8296,  0.9067], requires_grad=True),\n",
       " 124: tensor([ 1.2611, -2.0490], requires_grad=True),\n",
       " 125: tensor([ 0.8819, -0.4317], requires_grad=True),\n",
       " 126: tensor([-0.9188, -0.0537], requires_grad=True),\n",
       " 127: tensor([-0.0714, -1.3722], requires_grad=True),\n",
       " 128: tensor([ 1.5102, -1.9679], requires_grad=True),\n",
       " 129: tensor([1.4409, 0.0784], requires_grad=True),\n",
       " 130: tensor([0.3223, 0.0409], requires_grad=True),\n",
       " 131: tensor([ 0.2186, -0.7733], requires_grad=True),\n",
       " 132: tensor([0.5912, 1.2293], requires_grad=True),\n",
       " 133: tensor([-0.3713,  1.4012], requires_grad=True),\n",
       " 134: tensor([ 0.2676, -0.7691], requires_grad=True),\n",
       " 135: tensor([-1.6149, -0.6052], requires_grad=True),\n",
       " 136: tensor([-0.1068,  1.6549], requires_grad=True),\n",
       " 137: tensor([-0.5603, -0.9852], requires_grad=True),\n",
       " 138: tensor([ 0.0135, -1.4882], requires_grad=True),\n",
       " 139: tensor([ 3.5828, -0.0808], requires_grad=True),\n",
       " 140: tensor([ 0.7667, -0.5647], requires_grad=True),\n",
       " 141: tensor([-1.6076,  0.1197], requires_grad=True),\n",
       " 142: tensor([-2.0399,  1.1510], requires_grad=True),\n",
       " 143: tensor([ 0.1535, -0.9586], requires_grad=True),\n",
       " 144: tensor([-0.0929,  1.8242], requires_grad=True),\n",
       " 145: tensor([-1.2179,  2.2241], requires_grad=True),\n",
       " 146: tensor([ 0.9680, -0.6356], requires_grad=True),\n",
       " 147: tensor([-0.3318,  0.0909], requires_grad=True),\n",
       " 148: tensor([-0.0721, -0.3689], requires_grad=True),\n",
       " 149: tensor([-0.3410, -0.5790], requires_grad=True),\n",
       " 150: tensor([-1.0997, -1.1308], requires_grad=True),\n",
       " 151: tensor([ 0.9361, -0.4796], requires_grad=True),\n",
       " 152: tensor([0.7605, 1.1924], requires_grad=True),\n",
       " 153: tensor([1.9430, 0.5903], requires_grad=True),\n",
       " 154: tensor([-0.2343, -0.3952], requires_grad=True),\n",
       " 155: tensor([0.8901, 0.2031], requires_grad=True),\n",
       " 156: tensor([-1.9391,  1.3625], requires_grad=True),\n",
       " 157: tensor([2.3711, 0.5666], requires_grad=True),\n",
       " 158: tensor([0.4757, 2.1554], requires_grad=True),\n",
       " 159: tensor([ 0.9442, -1.0544], requires_grad=True),\n",
       " 160: tensor([-0.5540,  0.0168], requires_grad=True),\n",
       " 161: tensor([-0.2712,  0.1847], requires_grad=True),\n",
       " 162: tensor([-0.3898, -0.8187], requires_grad=True),\n",
       " 163: tensor([-1.2225,  0.8402], requires_grad=True),\n",
       " 164: tensor([-0.5122,  0.5142], requires_grad=True),\n",
       " 165: tensor([-0.6402,  0.6978], requires_grad=True),\n",
       " 166: tensor([-1.1260, -0.8421], requires_grad=True),\n",
       " 167: tensor([ 1.3413, -1.0248], requires_grad=True),\n",
       " 168: tensor([0.5644, 0.5216], requires_grad=True),\n",
       " 169: tensor([-0.2400, -0.8338], requires_grad=True),\n",
       " 170: tensor([0.2039, 1.9875], requires_grad=True),\n",
       " 171: tensor([-0.0579,  0.0477], requires_grad=True),\n",
       " 172: tensor([-1.4189,  0.2137], requires_grad=True),\n",
       " 173: tensor([0.5457, 1.1023], requires_grad=True),\n",
       " 174: tensor([-0.7845, -0.5503], requires_grad=True),\n",
       " 175: tensor([-1.6441, -1.0828], requires_grad=True),\n",
       " 176: tensor([0.3702, 0.4109], requires_grad=True),\n",
       " 177: tensor([-0.4448,  2.0180], requires_grad=True),\n",
       " 178: tensor([ 0.4113, -0.3352], requires_grad=True),\n",
       " 179: tensor([-0.6702, -1.2324], requires_grad=True),\n",
       " 180: tensor([-0.5769,  0.8905], requires_grad=True),\n",
       " 181: tensor([-1.1477, -0.0486], requires_grad=True),\n",
       " 182: tensor([1.0026, 0.8789], requires_grad=True),\n",
       " 183: tensor([1.6046, 0.0979], requires_grad=True),\n",
       " 184: tensor([-0.0907, -0.4466], requires_grad=True),\n",
       " 185: tensor([-2.3790,  2.1509], requires_grad=True),\n",
       " 186: tensor([0.4600, 0.5557], requires_grad=True),\n",
       " 187: tensor([0.5575, 0.0444], requires_grad=True),\n",
       " 188: tensor([-0.2681, -0.9439], requires_grad=True),\n",
       " 189: tensor([ 1.9034, -0.7142], requires_grad=True),\n",
       " 190: tensor([ 0.3571, -0.0037], requires_grad=True),\n",
       " 191: tensor([0.2737, 0.7221], requires_grad=True),\n",
       " 192: tensor([-1.1221, -1.1136], requires_grad=True),\n",
       " 193: tensor([ 0.4438, -0.8339], requires_grad=True),\n",
       " 194: tensor([ 1.6087, -0.5862], requires_grad=True),\n",
       " 195: tensor([-1.3945,  1.6699], requires_grad=True),\n",
       " 196: tensor([2.4091, 1.8045], requires_grad=True),\n",
       " 197: tensor([-1.7098,  0.4799], requires_grad=True),\n",
       " 198: tensor([-1.8254, -0.1563], requires_grad=True),\n",
       " 199: tensor([ 0.5537, -0.6873], requires_grad=True),\n",
       " 200: tensor([1.3311, 1.0543], requires_grad=True),\n",
       " 201: tensor([-1.8361, -0.2896], requires_grad=True),\n",
       " 202: tensor([0.2804, 0.1272], requires_grad=True),\n",
       " 203: tensor([-1.2778,  0.9146], requires_grad=True),\n",
       " 204: tensor([1.2314, 1.0571], requires_grad=True),\n",
       " 205: tensor([0.4386, 0.5390], requires_grad=True),\n",
       " 206: tensor([-0.0527,  0.2004], requires_grad=True),\n",
       " 207: tensor([ 0.7238, -3.1989], requires_grad=True),\n",
       " 208: tensor([-1.2170, -0.5537], requires_grad=True),\n",
       " 209: tensor([2.1193, 1.2310], requires_grad=True),\n",
       " 210: tensor([-1.4049,  0.8054], requires_grad=True),\n",
       " 211: tensor([-0.0396,  0.8781], requires_grad=True),\n",
       " 212: tensor([-0.2811,  0.8489], requires_grad=True),\n",
       " 213: tensor([-1.4055, -0.1174], requires_grad=True),\n",
       " 214: tensor([1.0152, 1.1685], requires_grad=True),\n",
       " 215: tensor([-0.0467,  0.7893], requires_grad=True),\n",
       " 216: tensor([-0.7121,  1.9689], requires_grad=True),\n",
       " 217: tensor([1.8716, 0.6628], requires_grad=True),\n",
       " 218: tensor([0.0203, 2.7299], requires_grad=True),\n",
       " 219: tensor([-2.7495, -0.8917], requires_grad=True),\n",
       " 220: tensor([ 1.0041, -0.5332], requires_grad=True),\n",
       " 221: tensor([-0.3945, -2.0815], requires_grad=True),\n",
       " 222: tensor([ 0.5308, -0.1214], requires_grad=True),\n",
       " 223: tensor([0.9370, 0.5470], requires_grad=True),\n",
       " 224: tensor([0.3297, 0.5251], requires_grad=True),\n",
       " 225: tensor([0.8226, 0.1355], requires_grad=True),\n",
       " 226: tensor([0.0173, 0.5830], requires_grad=True),\n",
       " 227: tensor([-0.7467, -0.8736], requires_grad=True),\n",
       " 228: tensor([-0.8360, -0.3396], requires_grad=True),\n",
       " 229: tensor([1.2472, 1.5014], requires_grad=True),\n",
       " 230: tensor([-0.4390,  0.6145], requires_grad=True),\n",
       " 231: tensor([-0.2683, -1.7034], requires_grad=True),\n",
       " 232: tensor([-0.6353,  0.4058], requires_grad=True),\n",
       " 233: tensor([-1.2834,  0.9334], requires_grad=True),\n",
       " 234: tensor([-0.4955,  1.0647], requires_grad=True),\n",
       " 235: tensor([-0.2430,  0.2267], requires_grad=True),\n",
       " 236: tensor([ 0.1249, -0.0554], requires_grad=True),\n",
       " 237: tensor([ 0.1232, -1.0004], requires_grad=True),\n",
       " 238: tensor([-1.0132,  0.7529], requires_grad=True),\n",
       " 239: tensor([-0.2479,  0.1080], requires_grad=True),\n",
       " 240: tensor([-1.1931,  0.9312], requires_grad=True),\n",
       " 241: tensor([-1.8791, -1.6467], requires_grad=True),\n",
       " 242: tensor([1.5052, 0.4238], requires_grad=True),\n",
       " 243: tensor([-0.3745,  0.7845], requires_grad=True),\n",
       " 244: tensor([1.0699, 0.7596], requires_grad=True),\n",
       " 245: tensor([-1.5224, -0.6092], requires_grad=True),\n",
       " 246: tensor([ 0.4249, -0.9940], requires_grad=True),\n",
       " 247: tensor([-1.4024,  0.0954], requires_grad=True),\n",
       " 248: tensor([-0.4547,  0.3629], requires_grad=True),\n",
       " 249: tensor([ 1.4016, -0.9025], requires_grad=True),\n",
       " 250: tensor([1.0091, 0.6611], requires_grad=True),\n",
       " 251: tensor([-0.3300,  1.3510], requires_grad=True),\n",
       " 252: tensor([-1.2577,  0.7598], requires_grad=True),\n",
       " 253: tensor([ 1.0184, -0.4426], requires_grad=True),\n",
       " 254: tensor([-1.7578,  1.8319], requires_grad=True),\n",
       " 255: tensor([-1.1863,  1.0722], requires_grad=True),\n",
       " 256: tensor([ 1.3804, -1.1997], requires_grad=True),\n",
       " 257: tensor([-1.1868, -1.7977], requires_grad=True),\n",
       " 258: tensor([1.6794, 0.2936], requires_grad=True),\n",
       " 259: tensor([ 1.2031, -1.1998], requires_grad=True),\n",
       " 260: tensor([ 0.1877, -0.8855], requires_grad=True),\n",
       " 261: tensor([-0.9968,  1.5002], requires_grad=True),\n",
       " 262: tensor([ 1.0367, -2.4923], requires_grad=True),\n",
       " 263: tensor([-2.3734, -1.3209], requires_grad=True),\n",
       " 264: tensor([ 0.4736, -0.5478], requires_grad=True),\n",
       " 265: tensor([2.4936, 0.2856], requires_grad=True),\n",
       " 266: tensor([-0.1338,  0.2922], requires_grad=True),\n",
       " 267: tensor([ 1.2129, -0.0769], requires_grad=True),\n",
       " 268: tensor([-1.7615,  0.0451], requires_grad=True),\n",
       " 269: tensor([0.0227, 0.3565], requires_grad=True),\n",
       " 270: tensor([0.7421, 0.8136], requires_grad=True),\n",
       " 271: tensor([-0.7852, -0.3636], requires_grad=True),\n",
       " 272: tensor([ 0.0993, -0.1024], requires_grad=True),\n",
       " 273: tensor([0.7812, 0.9047], requires_grad=True),\n",
       " 274: tensor([-1.1417,  0.5779], requires_grad=True),\n",
       " 275: tensor([-0.2260, -0.0162], requires_grad=True),\n",
       " 276: tensor([0.5695, 0.4138], requires_grad=True),\n",
       " 277: tensor([-0.0671, -1.9251], requires_grad=True),\n",
       " 278: tensor([3.1745, 0.7306], requires_grad=True),\n",
       " 279: tensor([0.2207, 0.8398], requires_grad=True),\n",
       " 280: tensor([-1.0629,  1.4699], requires_grad=True),\n",
       " 281: tensor([-0.0060,  1.1352], requires_grad=True),\n",
       " 282: tensor([0.0202, 0.4089], requires_grad=True),\n",
       " 283: tensor([-2.3217,  0.2405], requires_grad=True),\n",
       " 284: tensor([-0.4856,  0.9817], requires_grad=True),\n",
       " 285: tensor([-0.6318, -0.8845], requires_grad=True),\n",
       " 286: tensor([-0.0803, -0.4835], requires_grad=True),\n",
       " 287: tensor([-0.4543,  0.9728], requires_grad=True),\n",
       " 288: tensor([-2.2177,  0.1295], requires_grad=True),\n",
       " 289: tensor([ 0.3954, -0.1057], requires_grad=True),\n",
       " 290: tensor([0.3453, 0.3838], requires_grad=True),\n",
       " 291: tensor([ 0.8890, -0.1413], requires_grad=True),\n",
       " 292: tensor([ 1.5975, -1.8836], requires_grad=True),\n",
       " 293: tensor([-1.1824, -1.8588], requires_grad=True),\n",
       " 294: tensor([-0.5525,  0.0561], requires_grad=True),\n",
       " 295: tensor([ 1.6882, -1.2750], requires_grad=True),\n",
       " 296: tensor([ 0.1387, -1.3605], requires_grad=True),\n",
       " 297: tensor([ 0.3188, -1.1116], requires_grad=True),\n",
       " 298: tensor([-1.4265,  2.8578], requires_grad=True),\n",
       " 299: tensor([0.6696, 0.4701], requires_grad=True),\n",
       " 300: tensor([-1.3076,  0.7784], requires_grad=True),\n",
       " 301: tensor([ 1.9259, -2.2887], requires_grad=True),\n",
       " 302: tensor([-2.4218, -1.1294], requires_grad=True),\n",
       " 303: tensor([0.2482, 1.5061], requires_grad=True),\n",
       " 304: tensor([-1.0210, -0.6659], requires_grad=True),\n",
       " 305: tensor([2.4445, 0.6149], requires_grad=True),\n",
       " 306: tensor([-0.0127,  1.2290], requires_grad=True),\n",
       " 307: tensor([ 1.0109, -0.6360], requires_grad=True),\n",
       " 308: tensor([0.9106, 0.7581], requires_grad=True),\n",
       " 309: tensor([0.2689, 1.3230], requires_grad=True),\n",
       " 310: tensor([-0.9840,  1.2558], requires_grad=True),\n",
       " 311: tensor([ 0.5676, -1.3607], requires_grad=True),\n",
       " 312: tensor([-0.0643, -0.5968], requires_grad=True),\n",
       " 313: tensor([ 0.0675, -0.0927], requires_grad=True),\n",
       " 314: tensor([-0.8638,  1.0195], requires_grad=True),\n",
       " 315: tensor([-1.2100,  0.8693], requires_grad=True),\n",
       " 316: tensor([2.1377, 2.4060], requires_grad=True),\n",
       " 317: tensor([ 1.0907, -1.3488], requires_grad=True),\n",
       " 318: tensor([-1.0258,  1.8617], requires_grad=True),\n",
       " 319: tensor([1.4583, 1.8412], requires_grad=True),\n",
       " 320: tensor([-0.2529, -0.0251], requires_grad=True),\n",
       " 321: tensor([-0.7561, -0.9352], requires_grad=True),\n",
       " 322: tensor([-1.2891,  1.7433], requires_grad=True),\n",
       " 323: tensor([-0.6119, -1.6683], requires_grad=True),\n",
       " 324: tensor([-1.4067,  0.1574], requires_grad=True),\n",
       " 325: tensor([-0.0276,  0.2283], requires_grad=True),\n",
       " 326: tensor([-0.5673, -0.2496], requires_grad=True),\n",
       " 327: tensor([ 0.1276, -0.2198], requires_grad=True),\n",
       " 328: tensor([-1.2250, -0.4166], requires_grad=True),\n",
       " 329: tensor([-0.1857, -0.7976], requires_grad=True),\n",
       " 330: tensor([-1.6117, -0.6487], requires_grad=True),\n",
       " 331: tensor([1.9091, 0.2117], requires_grad=True),\n",
       " 332: tensor([1.0780, 0.5563], requires_grad=True),\n",
       " 333: tensor([0.5714, 0.1751], requires_grad=True),\n",
       " 334: tensor([-0.8086, -0.6384], requires_grad=True),\n",
       " 335: tensor([-0.4834, -0.7650], requires_grad=True),\n",
       " 336: tensor([0.9485, 0.0892], requires_grad=True),\n",
       " 337: tensor([-1.7747,  0.5739], requires_grad=True),\n",
       " 338: tensor([ 0.2543, -0.0415], requires_grad=True),\n",
       " 339: tensor([-1.3074, -0.1207], requires_grad=True),\n",
       " 340: tensor([1.7532, 0.9667], requires_grad=True),\n",
       " 341: tensor([-1.1922, -0.4680], requires_grad=True),\n",
       " 342: tensor([ 0.4294, -0.5129], requires_grad=True),\n",
       " 343: tensor([ 0.9894, -0.4650], requires_grad=True),\n",
       " 344: tensor([ 0.8624, -0.1984], requires_grad=True),\n",
       " 345: tensor([-1.5019, -0.5614], requires_grad=True),\n",
       " 346: tensor([2.8346, 0.2282], requires_grad=True),\n",
       " 347: tensor([ 0.5406, -0.5048], requires_grad=True),\n",
       " 348: tensor([-0.7389, -0.3698], requires_grad=True)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "torch.load(\"/home/william/Documents/DTU/GraphML_Bachelorprojekt/dataset/ogbn_mag/processed/venue_embeddings.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
